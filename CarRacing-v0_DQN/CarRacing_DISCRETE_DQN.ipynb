{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    SOURCE:\\n            https://levelup.gitconnected.com/dqn-from-scratch-with-tensorflow-2-eb0541151049\\n            https://towardsdatascience.com/explaining-double-q-learning-for-openai-environments-using-the-movie-tenet-816dc952f41c\\n            https://github.com/perseus784/Vehicle_Overtake_Double_DQN\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    SOURCE:\n",
    "            https://levelup.gitconnected.com/dqn-from-scratch-with-tensorflow-2-eb0541151049\n",
    "            https://towardsdatascience.com/explaining-double-q-learning-for-openai-environments-using-the-movie-tenet-816dc952f41c\n",
    "            https://github.com/perseus784/Vehicle_Overtake_Double_DQN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer Stores and retrieves gameplay experiences\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.gameplay_experiences = deque(maxlen=100000)\n",
    "    \n",
    "    def store_gameplay_experience(self, state, next_state, reward, action, done):\n",
    "        \"\"\"\n",
    "        Records a single step (state transition) of gameplay experience.\n",
    "        :param state: the current game state\n",
    "        :param next_state: the game state after taking action\n",
    "        :param reward: the reward taking action at the current state brings\n",
    "        :param action: the action taken at the current state\n",
    "        :param done: a boolean indicating if the game is finished after\n",
    "        taking the action\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.gameplay_experiences.append((state, next_state, reward, action, done))\n",
    "    \n",
    "    def sample_gameplay_batch(self):\n",
    "        \"\"\"\n",
    "        Samples a batch of gameplay experiences for training.\n",
    "        :return: a list of gameplay experiences\n",
    "        \"\"\"\n",
    "        batch_size = min(128, len(self.gameplay_experiences))\n",
    "        sampled_gameplay_batch = random.sample(self.gameplay_experiences, batch_size)\n",
    "        state_batch = []\n",
    "        next_state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        for gameplay_experience in sampled_gameplay_batch:\n",
    "            state_batch.append(gameplay_experience[0])\n",
    "            next_state_batch.append(gameplay_experience[1])\n",
    "            reward_batch.append(gameplay_experience[2])\n",
    "            action_batch.append(gameplay_experience[3])\n",
    "            done_batch.append(gameplay_experience[4])\n",
    "            \n",
    "        return np.array(state_batch), np.array(next_state_batch), np.array(\n",
    "            action_batch), np.array(reward_batch), np.array(done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, MaxPool2D, Dropout, Input, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "IM_HEIGHT = 96\n",
    "IM_WIDTH = 96\n",
    "IM_CHANNEL = 3\n",
    "NUM_ACTIONS = 8\n",
    "LEARNING_RATE = .001\n",
    "EXPLORATION_RATE = .05\n",
    "GAMMA = .95\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent\n",
    "    The agent that explores the game and learn how to play the game by\n",
    "    learning how to predict the expected long-term return, the Q value given\n",
    "    a state-action pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.q_net = self._build_dqn_model()\n",
    "        self.target_q_net = self._build_dqn_model()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _build_dqn_model():\n",
    "        \"\"\"\n",
    "        Builds a deep neural net which predicts the Q values for all possible\n",
    "        actions given a state. The input should have the shape of the state, and\n",
    "        the output should have the same shape as the action space since we want\n",
    "        1 Q value per possible action.\n",
    "        :return: Q network\n",
    "        \"\"\"\n",
    "        inp = Input((IM_HEIGHT, IM_WIDTH, IM_CHANNEL))\n",
    "        \n",
    "        x = Conv2D(32, (3,3), activation='relu')(inp)\n",
    "        x = Conv2D(32, (3,3), activation='relu')(x)\n",
    "        x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "        x = MaxPool2D((2,2))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        x = Conv2D(64, (3,3), activation='relu')(x)\n",
    "        x = Conv2D(128, (3,3), activation='relu')(x)\n",
    "        x = MaxPool2D((2,2))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        x = Conv2D(128, (3,3), activation='relu')(x)\n",
    "        x = Conv2D(256, (3,3), activation='relu')(x)\n",
    "        x = MaxPool2D((2,2))(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        x = Conv2D(512, (3,3), activation='relu')(x)\n",
    "        x = MaxPool2D((2,2))(x)\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dense(NUM_ACTIONS, activation='linear')(x)\n",
    "\n",
    "        q_net = Model(inputs=inp, outputs=x)\n",
    "        q_net.summary()\n",
    "        \n",
    "        q_net.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
    "        \n",
    "        return q_net\n",
    "    \n",
    "    def map_int_to_action(self, n):\n",
    "        \"\"\"\n",
    "        Maps the integer value to an action in env.action_space values for CarRacing-v0\n",
    "        :param n: an integer value\n",
    "        :return: action\n",
    "        \"\"\"\n",
    "        discrete_action_space = {\n",
    "            \"turn_left\":[-1,0,0],\n",
    "            \"turn_right\":[1,0,0],\n",
    "            \"go\":[0,1,0],\n",
    "            \"go_left\":[-1,1,0],\n",
    "            \"go_right\":[1,1,0],\n",
    "            \"brake\":[0,0,1],\n",
    "            \"brake_left\":[-1,0,1],\n",
    "            \"brake_right\":[1,0,1]\n",
    "        }\n",
    "        discrete_actions = list(discrete_action_space.values())\n",
    "        action = (discrete_actions[n])\n",
    "        return action\n",
    "\n",
    "    def random_policy(self, num_actions=NUM_ACTIONS):\n",
    "        \"\"\"\n",
    "        Outputs a random action\n",
    "        :param num_actions: number of actions in the env.action_space\n",
    "        :return: action\n",
    "        \"\"\"\n",
    "        n = np.random.randint(num_actions)\n",
    "        action = self.map_int_to_action(n)\n",
    "        return action\n",
    "\n",
    "    def collect_policy(self, state, num_actions=NUM_ACTIONS, exploration_rate=EXPLORATION_RATE):\n",
    "        \"\"\"\n",
    "        Similar to policy but with some randomness to encourage exploration.\n",
    "        :param state: the game state\n",
    "        :param num_actions: number of actions in the env.action_space\n",
    "        :return: action\n",
    "        \"\"\"\n",
    "        if np.random.random() < exploration_rate:\n",
    "            return self.random_policy(num_actions)\n",
    "        return self.policy(state)\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Takes a state from the game environment and returns an action that\n",
    "        has the highest Q value and should be taken as the next step.\n",
    "        i.e. run the state through the q_net and take the action which is the index that has the highest Q value\n",
    "        :param state: the current game environment state\n",
    "        :return: an action\n",
    "        \"\"\"\n",
    "        state_input = tf.convert_to_tensor(state[None, :], dtype=tf.float32)\n",
    "        action_q = self.target_q_net(state_input)\n",
    "        best_action = np.argmax(action_q.numpy()[0], axis=0)\n",
    "        \n",
    "        # if you are using CarRacing-v0 environment with discrete actions,\n",
    "        # you have to map the int value to [int, int, int]\n",
    "        # Creating discrete actions\n",
    "        best_action = self.map_int_to_action(best_action)\n",
    "        #print(best_action)\n",
    "        \n",
    "        return best_action\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Updates the current target_q_net with the q_net which brings all the\n",
    "        training in the q_net to the target_q_net.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.target_q_net.set_weights(self.q_net.get_weights())\n",
    "\n",
    "    def train(self, batch):\n",
    "        \"\"\"\n",
    "        Trains the underlying network with a batch of gameplay experiences to\n",
    "        help it better predict the Q values.\n",
    "        :param batch: a batch of gameplay experiences\n",
    "        :return: training loss\n",
    "        \"\"\"\n",
    "        # training the model from the feed coming from the replay batch\n",
    "        state_batch, next_state_batch, action_batch, reward_batch, done_batch = batch\n",
    "        current_q_values = self.q_net(state_batch).numpy()\n",
    "        # note that in DQN, only q-value of the actions that were taking will be updated\n",
    "        # hence copying the q_value for the actions that were not taken in target_q_values\n",
    "        target_q_values = np.copy(current_q_values)\n",
    "        # getting the max Q values of the states after transition by running the next_state through target_q_net\n",
    "        # and taking the max of Q values for all actions for each sample\n",
    "        next_q_values = self.target_q_net(next_state_batch).numpy()\n",
    "        max_next_q_values = np.amax(next_q_values, axis=1)\n",
    "        for i in range(state_batch.shape[0]):\n",
    "            # updating the q_value of the action taken with the max_q_values of the next_state plus the\n",
    "            # intermediate reward from the action taken\n",
    "            target_q_val = reward_batch[i]\n",
    "            if not done_batch[i]:\n",
    "                target_q_val += GAMMA * max_next_q_values[i]\n",
    "            target_q_values[i][action_batch[i]] = target_q_val\n",
    "        # training the q_net with the target_q_values\n",
    "        training_history = self.q_net.fit(x=state_batch, y=target_q_values, verbose=0)\n",
    "        loss = training_history.history['loss']\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 94, 94, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 92, 92, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 90, 90, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 43, 43, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 41, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 4,388,904\n",
      "Trainable params: 4,388,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 96, 96, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 94, 94, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 92, 92, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 90, 90, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 43, 43, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 41, 41, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 4,388,904\n",
      "Trainable params: 4,388,904\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blwme/anaconda3/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "Track generation: 1217..1525 -> 308-tiles track\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Track generation: 1236..1549 -> 313-tiles track\n",
      "Track generation: 1128..1422 -> 294-tiles track\n",
      "Track generation: 1100..1379 -> 279-tiles track\n",
      "Track generation: 1053..1320 -> 267-tiles track\n",
      "Track generation: 1055..1323 -> 268-tiles track\n",
      "Track generation: 1228..1539 -> 311-tiles track\n",
      "Track generation: 1121..1405 -> 284-tiles track\n",
      "Track generation: 1197..1505 -> 308-tiles track\n",
      "episode: 1/2\tavg(reward): -74.43\tloss: 234.93\n",
      "Track generation: 1196..1508 -> 312-tiles track\n",
      "Track generation: 1156..1449 -> 293-tiles track\n",
      "Track generation: 1168..1464 -> 296-tiles track\n",
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Track generation: 1224..1534 -> 310-tiles track\n",
      "Track generation: 1004..1268 -> 264-tiles track\n",
      "Track generation: 1022..1288 -> 266-tiles track\n",
      "Track generation: 1264..1584 -> 320-tiles track\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "episode: 2/2\tavg(reward): -83.34\tloss: 0.08\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training loop\n",
    "This module trains the DQN agent by trial and error. In this module the DQN\n",
    "agent will play the game episode by episode, store the gameplay experiences\n",
    "and then use the saved gameplay experiences to train the underlying model.\n",
    "\"\"\"\n",
    "import gym\n",
    "\n",
    "\n",
    "def evaluate_training_result(env, agent, num_episodes=10):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the current DQN agent by using it to play a\n",
    "    few episodes of the game and then calculates the average reward it gets.\n",
    "    The higher the average reward is the better the DQN agent performs.\n",
    "    :param env: the game environment\n",
    "    :param agent: the DQN agent\n",
    "    :return: average reward across episodes\n",
    "    \"\"\"\n",
    "    total_reward = 0.\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.\n",
    "        while not done:\n",
    "            action = agent.policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        total_reward += episode_reward\n",
    "    average_reward = total_reward / num_episodes\n",
    "    return average_reward\n",
    "\n",
    "\n",
    "def collect_gameplay_experiences(env, agent, buffer):\n",
    "    \"\"\"\n",
    "    Collects gameplay experiences by playing env with the instructions\n",
    "    produced by agent and stores the gameplay experiences in buffer.\n",
    "    :param env: the game environment\n",
    "    :param agent: the DQN agent\n",
    "    :param buffer: the replay buffer\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.collect_policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            reward = -1.0\n",
    "        buffer.store_gameplay_experience(state, next_state,\n",
    "                                         reward, action, done)\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "def train_model(max_episodes=50000, update_indicator=10, visualize=False):\n",
    "    \"\"\"\n",
    "    Trains a DQN agent to play the highway-v0 game by trial and error\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    agent = DQNAgent()\n",
    "    buffer = ReplayBuffer()\n",
    "    env = gym.make('CarRacing-v0')\n",
    "    for episode in range(1, max_episodes+1):\n",
    "        collect_gameplay_experiences(env, agent, buffer)\n",
    "        gameplay_experience_batch = buffer.sample_gameplay_batch()\n",
    "        loss = agent.train(gameplay_experience_batch)\n",
    "        average_reward = evaluate_training_result(env, agent)\n",
    "        print('episode: %d/%d\\tavg(reward): %.2f\\tloss: %.2f'%(episode, max_episodes, average_reward, loss[0]))\n",
    "        if episode % update_indicator == 0:\n",
    "            agent.update_target_network()\n",
    "    env.close()\n",
    "        \n",
    "\n",
    "\n",
    "train_model(max_episodes=2, update_indicator=1, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
