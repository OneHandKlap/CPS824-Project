{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import gc\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import imageio\n",
    "import base64\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "from torchsummary import summary ######## pip install torch-summary ######## DELETE ########\n",
    "\n",
    "\n",
    "def memory_used():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss * 1e-6  # To megabyte\n",
    "\n",
    "'''\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "'''\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MEMORY_CAPACITY = 7000\n",
    "NUM_TRAINING_EPISODES = 50\n",
    "MAX_EPISODE_TIME = 1000\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "ENV_CLEAR = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children Counter:  0  Layer Name:  conv1\n",
      "Children Counter:  1  Layer Name:  bn1\n",
      "Children Counter:  2  Layer Name:  relu\n",
      "Children Counter:  3  Layer Name:  maxpool\n",
      "Children Counter:  4  Layer Name:  layer1\n",
      "Children Counter:  5  Layer Name:  layer2\n",
      "Children Counter:  6  Layer Name:  layer3\n",
      "Children Counter:  7  Layer Name:  layer4\n",
      "Children Counter:  8  Layer Name:  avgpool\n",
      "Children Counter:  9  Layer Name:  fc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1',\n",
       "              Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)),\n",
       "             ('bn1',\n",
       "              BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n",
       "             ('relu', ReLU(inplace=True)),\n",
       "             ('maxpool',\n",
       "              MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)),\n",
       "             ('layer1',\n",
       "              Sequential(\n",
       "                (0): Bottleneck(\n",
       "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                  (downsample): Sequential(\n",
       "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (1): Bottleneck(\n",
       "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (2): Bottleneck(\n",
       "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "              )),\n",
       "             ('layer2',\n",
       "              Sequential(\n",
       "                (0): Bottleneck(\n",
       "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                  (downsample): Sequential(\n",
       "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (1): Bottleneck(\n",
       "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (2): Bottleneck(\n",
       "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (3): Bottleneck(\n",
       "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "              )),\n",
       "             ('layer3',\n",
       "              Sequential(\n",
       "                (0): Bottleneck(\n",
       "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                  (downsample): Sequential(\n",
       "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (1): Bottleneck(\n",
       "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (2): Bottleneck(\n",
       "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (3): Bottleneck(\n",
       "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (4): Bottleneck(\n",
       "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (5): Bottleneck(\n",
       "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "              )),\n",
       "             ('layer4',\n",
       "              Sequential(\n",
       "                (0): Bottleneck(\n",
       "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                  (downsample): Sequential(\n",
       "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (1): Bottleneck(\n",
       "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (2): Bottleneck(\n",
       "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "              )),\n",
       "             ('avgpool', AdaptiveAvgPool2d(output_size=(1, 1))),\n",
       "             ('fc', Linear(in_features=2048, out_features=1000, bias=True))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### RESNET #########\n",
    "# SOURCE: https://medium.com/the-owl/extracting-features-from-an-intermediate-layer-of-a-pretrained-model-in-pytorch-c00589bda32b\n",
    "\n",
    "rn50 = models.resnet50(pretrained=True) ##### Expects 4-dimensional input for 4-dimensional weight [64, 3, 7, 7]\n",
    "children_counter = 0\n",
    "for n,c in rn18.named_children():\n",
    "    print(\"Children Counter: \",children_counter,\" Layer Name: \",n,)\n",
    "    children_counter+=1\n",
    "\n",
    "class rn18_feature_extractor(nn.Module):\n",
    "    def __init__(self,output_layer = None):\n",
    "        super().__init__()\n",
    "        self.pretrained = models.resnet18(pretrained=True)\n",
    "        self.output_layer = output_layer\n",
    "        self.layers = list(self.pretrained._modules.keys())\n",
    "        self.layer_count = 0\n",
    "        for l in self.layers:\n",
    "            if l != self.output_layer:\n",
    "                self.layer_count += 1\n",
    "            else:\n",
    "                break\n",
    "        for i in range(1,len(self.layers)-self.layer_count):\n",
    "            self.dummy_var = self.pretrained._modules.pop(self.layers[-i])\n",
    "        self.dummy_var= None ##### freeing up some space\n",
    "        \n",
    "        self.net = nn.Sequential(self.pretrained._modules)\n",
    "        self.pretrained = None ##### freeing up some space\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "rn18_fe = rn18_feature_extractor(output_layer='layer4').to(device)\n",
    "#img = torch.Tensor(1, 3, 60, 40) ##### returns torch.Size([1, 64, 15, 10]) (1, 512, h, w)\n",
    "#features = rn18_fe(img)\n",
    "#print(features.shape)\n",
    "rn50._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       9,408\n",
      "|    └─BatchNorm2d: 2-2                  128\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─MaxPool2d: 2-4                    --\n",
      "|    └─Sequential: 2-5                   --\n",
      "|    |    └─BasicBlock: 3-1              73,984\n",
      "|    |    └─BasicBlock: 3-2              73,984\n",
      "|    └─Sequential: 2-6                   --\n",
      "|    |    └─BasicBlock: 3-3              230,144\n",
      "|    |    └─BasicBlock: 3-4              295,424\n",
      "|    └─Sequential: 2-7                   --\n",
      "|    |    └─BasicBlock: 3-5              919,040\n",
      "|    |    └─BasicBlock: 3-6              1,180,672\n",
      "|    └─Sequential: 2-8                   --\n",
      "|    |    └─BasicBlock: 3-7              3,673,088\n",
      "|    |    └─BasicBlock: 3-8              4,720,640\n",
      "=================================================================\n",
      "Total params: 11,176,512\n",
      "Trainable params: 11,176,512\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pretrained', None),\n",
       "             ('dummy_var', None),\n",
       "             ('net', Sequential(\n",
       "                (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "                (layer1): Sequential(\n",
       "                  (0): BasicBlock(\n",
       "                    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                  (1): BasicBlock(\n",
       "                    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (layer2): Sequential(\n",
       "                  (0): BasicBlock(\n",
       "                    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (downsample): Sequential(\n",
       "                      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): BasicBlock(\n",
       "                    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (layer3): Sequential(\n",
       "                  (0): BasicBlock(\n",
       "                    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (downsample): Sequential(\n",
       "                      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): BasicBlock(\n",
       "                    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "                (layer4): Sequential(\n",
       "                  (0): BasicBlock(\n",
       "                    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (downsample): Sequential(\n",
       "                      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): BasicBlock(\n",
       "                    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    (relu): ReLU(inplace=True)\n",
       "                    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  )\n",
       "                )\n",
       "              ))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(rn18_fe, input_size=(1, 3, 60, 40))\n",
    "#print('*'*80)\n",
    "rn18_fe._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        \"\"\"\n",
    "            :param h is the height of the image (in this case 60)\n",
    "            :param w is the height of the image (in this case 40)\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        #self.bn1 = nn.BatchNorm2d(16)\n",
    "        #self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "        #self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        #self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        \n",
    "        ##### DELETE #####\n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        #--------------- USING BASIC ARCHITECTURE ---------------\n",
    "        #self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2)\n",
    "        #self.bn1 = nn.BatchNorm2d(64)\n",
    "        #self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(128)\n",
    "        #self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2)\n",
    "        #self.bn3 = nn.BatchNorm2d(256)\n",
    "        #linear_input_size = conv2d_size_out(conv2d_size_out(conv2d_size_out(w))) * conv2d_size_out(conv2d_size_out(conv2d_size_out(h))) * 256\n",
    "        #print(linear_input_size)\n",
    "        #self.dense1 = nn.Linear(linear_input_size, 1024)\n",
    "        #self.dense2 = nn.Linear(1024, 512)\n",
    "        #self.head = nn.Linear(512, outputs)\n",
    "        \n",
    "        \n",
    "        #--------------- USING PRE-TRAINED RESNET18 ---------------\n",
    "        #self.rn18_fe = rn18_feature_extractor(output_layer='maxpool') removing rn18 from dqn to avoid optimizing it\n",
    "        #self.conv1 = nn.Conv2d(512, 512, kernel_size=5, stride=2)\n",
    "        #self.bn1 = nn.BatchNorm2d(512)\n",
    "        #self.conv2 = nn.Conv2d(512, 1024, kernel_size=3, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(512)\n",
    "        #self.conv3 = nn.Conv2d(256, 512, kernel_size=5, stride=1)\n",
    "        #linear_input_size = conv2d_size_out(conv2d_size_out(conv2d_size_out(w))) * conv2d_size_out(conv2d_size_out(conv2d_size_out(h))) * 64\n",
    "        #print(linear_input_size)\n",
    "        self.dense1 = nn.Linear(2048, 1024)\n",
    "        self.dense2 = nn.Linear(1024, 512)\n",
    "        self.head = nn.Linear(512, outputs)\n",
    "        \n",
    "        ##### DELETE #####\n",
    "\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        #def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "        #    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        #convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        #convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        #linear_input_size = convw * convh * 32 ##### linear_input_size = 256\n",
    "        #self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            #--------------- USING BASIC ARCHITECTURE ---------------\n",
    "            #x = F.relu(self.bn1(self.conv1(x)))\n",
    "            #print('conv1.shape = ', x.shape)\n",
    "            #x = F.relu(self.bn2(self.conv2(x)))\n",
    "            #print('conv2.shape = ', x.shape)\n",
    "            #x = F.relu(self.bn3(self.conv3(x)))\n",
    "            #print('conv3.shape = ', x.shape)\n",
    "            \n",
    "            ##### DELETE #####\n",
    "            #x = x.view(x.size(0), -1) ##### Flattening ##### x = torch.flatten(x, 1)\n",
    "            #print('view = ', x.shape)\n",
    "            #x = F.relu(self.dense1(x))\n",
    "            #print('dense1.shape = ', x.shape)\n",
    "            #x = F.relu(self.dense2(x))\n",
    "            #print('dense2.shape = ', x.shape)\n",
    "            #output = self.head(x)\n",
    "            #print('output = ', output)\n",
    "            #return output\n",
    "            \n",
    "            \n",
    "            #--------------- USING PRE-TRAINED RESNET18 ---------------\n",
    "            #print('init.shape =',x.shape)\n",
    "            #x = self.rn18_fe.forward(x) ##### output shape is torch.Size([1, 64, 15, 10]) removing it to avoid optimizing\n",
    "            #print('resnet18.shape =',x.shape)\n",
    "            #x = F.relu(self.bn1(self.conv1(x)))\n",
    "            #print('conv1.shape = ', x.shape)\n",
    "            #x = F.relu(self.bn2(self.conv2(x)))\n",
    "            #print('conv2.shape = ', x.shape)\n",
    "            \n",
    "            #x = F.relu(self.bn3(self.conv3(x)))\n",
    "            #print('conv3.shape = ', x.shape)\n",
    "            \n",
    "            #x = x.view(x.size(0), -1)\n",
    "            #print('view = ', x.shape)\n",
    "            x = F.relu(self.dense1(x))\n",
    "            print('dense1.shape = ', x.shape)\n",
    "            x = F.relu(self.dense2(x))\n",
    "            print('dense2.shape = ', x.shape)\n",
    "            output = self.head(x)\n",
    "            print('output = ', output)\n",
    "            return output\n",
    "            \n",
    "            ##### DELETE #####\n",
    "            \n",
    "            #return self.head(x.view(x.size(0), -1)) ####### [batch_size, Ch*H*W] i.e. FLATENNING\n",
    "        except:\n",
    "            print(x)\n",
    "            print('--------------- EXCEPTION ---------------')\n",
    "            #return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        # if len(self.memory) > self.capacity:\n",
    "        #    self.clear()\n",
    "\n",
    "        # self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def clear(self):\n",
    "        # print(\"CLEARING MEMORY\")\n",
    "        self.position = 0\n",
    "        for m in self.memory:\n",
    "            del m\n",
    "        del self.memory\n",
    "        self.memory = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Model Class for Training and Testing\n",
    "\n",
    "\n",
    "class RL_Model():\n",
    "\n",
    "    # Creates a new RL Model, given a Gym Environment,\n",
    "    # NeuralNetwork Class and optional Action Space\n",
    "    def __init__(self, env, nn, action_space, env_string=None):\n",
    "        # set env\n",
    "        self.env = env\n",
    "        if env_string:\n",
    "            self.env_string = env_string\n",
    "            self.env = gym.make(env_string).unwrapped\n",
    "\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # initialize screen and nn\n",
    "        self.env.reset()\n",
    "        _, _, screen_height, screen_width = self.get_screen().shape\n",
    "\n",
    "        # set action space\n",
    "        if(action_space):\n",
    "            self.action_space = action_space\n",
    "        else:\n",
    "            self.action_space = env.action_space\n",
    "\n",
    "        # policy net\n",
    "        self.policy = nn(screen_height, screen_width,\n",
    "                         len(self.action_space)).to(self.device) ##### Height = 60, Width = 40\n",
    "        summary(self.policy, input_size=(3, screen_height, screen_width)) ###### DELETE ######\n",
    "\n",
    "        # target net\n",
    "        self.target = nn(screen_height, screen_width,\n",
    "                         len(self.action_space)).to(self.device)\n",
    "        summary(self.target, input_size=(3, screen_height, screen_width)) ###### DELETE ######\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.RMSprop(self.policy.parameters())\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "        # variables for training\n",
    "        self.steps_taken = 0\n",
    "\n",
    "        self.episode_durations = []\n",
    "\n",
    "    # load policy-net weights\n",
    "    def load(self, path=\"rl_model_weights.pth\"):\n",
    "        checkpoint = torch.load(path)\n",
    "        # policy net\n",
    "        self.policy.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.policy.eval()\n",
    "\n",
    "        # target net\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # save policy-net weights\n",
    "    def save(self, path=\"rl_model_weights\"):\n",
    "        torch.save({\n",
    "            # 'epoch': epoch,\n",
    "            'model_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "        },\n",
    "            path + \".pth\")\n",
    "\n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array')\n",
    "        screen = screen[np.ix_([x for x in range(100, 400)], [\n",
    "                           x for x in range(200, 400)])]\n",
    "        screen = screen.transpose((2, 0, 1))\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 ######## screen.shape = (3x300x200)\n",
    "        screen = torch.from_numpy(screen) ######## torch.Size([3, 300, 200])\n",
    "        return resize(screen).unsqueeze(0).to(self.device) ####### torch.Size([1, 3, 60, 40])\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_taken / EPS_DECAY)\n",
    "        self.steps_taken += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(len(self.action_space))]], device=self.device, dtype=torch.long)\n",
    "\n",
    "    def select_deterministic_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            da = self.policy(state).max(1)[1].view(1, 1) ###### DELETE ######\n",
    "            #print('action =', da) ###### DELETE ######\n",
    "            return da ###### DELETE ######\n",
    "            #return self.policy(state).max(1)[1].view(1, 1)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(\n",
    "            lambda s: s is not None, batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None])\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        #extracted_features = rn18_fe.forward(state_batch)\n",
    "        #state_action_values = self.policy(extracted_features).gather(1, action_batch)\n",
    "        extracted_features = rn18_fe.forward(state_batch)\n",
    "        extracted_features = torch.flatten(extracted_features, 1)\n",
    "        state_action_values = self.policy(extracted_features).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target(\n",
    "            torch.flatten(rn18_fe.forward(non_final_next_states),1)).max(1)[0].detach()\n",
    "        #next_state_values[non_final_mask] = self.target(\n",
    "        #    non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (\n",
    "            next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # del non_final_mask\n",
    "        # del non_final_next_states\n",
    "        # del state_batch\n",
    "        # del action_batch\n",
    "        # del reward_batch\n",
    "        # del loss\n",
    "        # gc.collect()\n",
    "\n",
    "    def plot_durations(self):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    def train(self, num_episodes=NUM_TRAINING_EPISODES, epoch=1, render=False):\n",
    "        self.steps_taken = 0\n",
    "        acc_rewards = np.zeros(num_episodes)\n",
    "        plt_color = [(random.random(), random.random(), random.random())]\n",
    "        for i_ep in range(1, num_episodes+1):\n",
    "            #print(\"EPOCH: \" + str(epoch) + \" EPISODE: \" + str(i_ep))\n",
    "            #print(\"MEM ALLOCATED: \" + str(torch.cuda.memory_allocated()))\n",
    "            #print(\"MEM CACHE: \" + str(torch.cuda.memory_reserved()))\n",
    "            #print('Ram Used: %f' % memory_used())\n",
    "\n",
    "            # clear env\n",
    "            if i_ep % ENV_CLEAR == 0 and self.env_string:\n",
    "                self.env.close()\n",
    "                del self.env\n",
    "                self.env = gym.make(self.env_string).unwrapped\n",
    "\n",
    "            # reset env and state\n",
    "            self.env.reset()\n",
    "            last_screen = self.get_screen()\n",
    "            current_screen = self.get_screen()\n",
    "            state = current_screen - last_screen\n",
    "            \n",
    "            ### MEASUREMENTS ###\n",
    "            cntr = 0\n",
    "\n",
    "            for t in count():\n",
    "                if render:\n",
    "                    plt.imshow(self.get_screen().cpu().squeeze(\n",
    "                        0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "                    plt.draw()\n",
    "                    plt.pause(1e-3)\n",
    "\n",
    "                # select an action from the state\n",
    "                extracted_features = rn18_fe.forward(state)\n",
    "                extracted_features = torch.flatten(extracted_features, 1)\n",
    "                print('features =', extracted_features.shape)\n",
    "                action = self.select_action(extracted_features)\n",
    "                _, reward, done, _ = self.env.step(\n",
    "                    self.action_space[action.item()])\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "\n",
    "                # increase acc rewards\n",
    "                acc_rewards[i_ep-1] += reward\n",
    "\n",
    "                # observe new state\n",
    "                last_screen = current_screen\n",
    "                current_screen = self.get_screen()\n",
    "                # if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "                # else:\n",
    "                #    next_state = None\n",
    "\n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one optimization step on the target network\n",
    "                self.optimize_model()\n",
    "                \n",
    "                cntr += 1 #### \n",
    "\n",
    "                # Check if past step limit\n",
    "                if t > MAX_EPISODE_TIME:\n",
    "                    break\n",
    "\n",
    "            ### PRINTING PERFORMANCE ###\n",
    "            print('Episode:', i_ep, ' '*3,'Total_reward: %.2f' % acc_rewards[i_ep-1], ' '*3, \\\n",
    "                 'Average_RPE: %.2f' % (acc_rewards[i_ep-1] / cntr))\n",
    "            \n",
    "            \n",
    "            # Update target network, copy all weights and biases\n",
    "            if i_ep % TARGET_UPDATE == 0:\n",
    "                self.target.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "            # plot\n",
    "            #plt.title('Rewards Over Episode')\n",
    "            #plt.xlabel('Episode')\n",
    "            #plt.ylabel('Rewards')\n",
    "            #plt.xticks(range(1, epoch * (num_episodes+1)))\n",
    "            #plt.scatter(torch.tensor([i_ep + (epoch - 1) * num_episodes], dtype=torch.float), torch.tensor(\n",
    "            #    [max(acc_rewards[i_ep-1], -1000)], dtype=torch.float), c=plt_color, label=\"Epoch \" + str(epoch) if i_ep == 1 else '')\n",
    "            #plt.legend()\n",
    "            #plt.draw()\n",
    "            #plt.pause(1e-3)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        return (range(1, num_episodes+1), acc_rewards)\n",
    "\n",
    "    def generate_policy_video(self, filename=\"rl_model\", num_episodes=1, fps=30, max_episode_time=MAX_EPISODE_TIME):\n",
    "        filename = filename + \".mp4\"\n",
    "        with imageio.get_writer(filename, fps=fps) as video:\n",
    "            for episode in range(num_episodes):\n",
    "                time_step = self.env.reset()\n",
    "                done = False\n",
    "                video.append_data(self.env.render(mode=\"rgb_array\"))\n",
    "                last_screen = self.get_screen()\n",
    "                current_screen = self.get_screen()\n",
    "                state = current_screen - last_screen\n",
    "                \n",
    "\n",
    "                for i in range(max_episode_time):\n",
    "                    action = self.select_deterministic_action(state)\n",
    "                    #action = self.select_action(state)\n",
    "                    _, reward, done, _ = self.env.step(\n",
    "                        self.action_space[action.item()])\n",
    "                    video.append_data(self.env.render(mode=\"rgb_array\"))\n",
    "                    last_screen = current_screen\n",
    "                    current_screen = self.get_screen()\n",
    "                    state = current_screen-last_screen\n",
    "\n",
    "                    if(done):\n",
    "                        break\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1083..1358 -> 275-tiles track\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            2,098,176\n",
      "├─Linear: 1-2                            524,800\n",
      "├─Linear: 1-3                            4,104\n",
      "=================================================================\n",
      "Total params: 2,627,080\n",
      "Trainable params: 2,627,080\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            2,098,176\n",
      "├─Linear: 1-2                            524,800\n",
      "├─Linear: 1-3                            4,104\n",
      "=================================================================\n",
      "Total params: 2,627,080\n",
      "Trainable params: 2,627,080\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('CarRacing-v0').unwrapped\n",
    "\n",
    "discrete_action_space = {\"turn_left\": [-1, 0, 0], \"turn_right\": [1, 0, 0], \"go\": [0, 1, 0], \"go_left\": [-1,\n",
    "                                                                                                        1, 0], \"go_right\": [1, 1, 0], \"brake\": [0, 0, 1], \"brake_left\": [-1, 0, 1], \"brake_right\": [1, 0, 1]}\n",
    "d_actions = list(discrete_action_space.values())\n",
    "\n",
    "model = RL_Model(gym.make('CarRacing-v0').unwrapped,\n",
    "                 DQN, d_actions, 'CarRacing-v0')\n",
    "\n",
    "# model.generate_policy_video(\"rl_progress_ep_\" + str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1136..1431 -> 295-tiles track\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2454, -0.0732, -0.1279, -0.0880,  0.1029, -0.1309,  0.0319,  0.1829]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1499, -0.0460, -0.1106, -0.0596,  0.0351, -0.1043, -0.0469,  0.0973]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2953, -0.0098, -0.2049, -0.0883,  0.1702, -0.1544, -0.0883,  0.0510]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.3986,  0.0025, -0.0823,  0.0173,  0.1464, -0.3294, -0.0614,  0.0822]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1673, -0.0352, -0.0153,  0.1432,  0.0892, -0.0887, -0.1828,  0.0783]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2979,  0.0115, -0.0709,  0.0928, -0.0756, -0.1240, -0.0896,  0.1077]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2670, -0.0500, -0.0652, -0.0195, -0.0201, -0.2835, -0.0526,  0.0926]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1619, -0.0881, -0.0574, -0.0806,  0.0798, -0.1852, -0.0963,  0.1733]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.3568,  0.0079, -0.0868, -0.1025, -0.0005, -0.1653, -0.0842,  0.0118]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1964, -0.0620, -0.1543, -0.0251, -0.0022, -0.1096, -0.0681,  0.0644]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.4071, -0.0983, -0.1607,  0.0344,  0.0083, -0.1838, -0.0859,  0.0259]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2870, -0.0581, -0.0675, -0.1017,  0.0541, -0.1779, -0.0548,  0.1045]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.0790, -0.0679, -0.0850,  0.0385,  0.0561, -0.0308, -0.0704,  0.2494]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.0350, -0.1323,  0.0251,  0.0114, -0.0070, -0.1500, -0.2442,  0.1006]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.3187, -0.0460, -0.1235,  0.1228,  0.0072, -0.0696, -0.1390, -0.0345]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.3319, -0.0831, -0.0034, -0.0161, -0.0291, -0.2587, -0.1047,  0.2074]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2075, -0.1107,  0.0293, -0.1662, -0.0965, -0.2116, -0.0577,  0.0776]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2254,  0.0050, -0.1079,  0.0336, -0.1256, -0.1559, -0.0623,  0.0777]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2476, -0.0627, -0.1246,  0.1613,  0.1758, -0.2125, -0.1270,  0.1625]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1598, -0.0427, -0.1607, -0.0867,  0.0549, -0.2053, -0.1134,  0.0454]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2283, -0.0961, -0.0491, -0.0390, -0.0074, -0.1297, -0.2272,  0.0842]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2387, -0.0289, -0.1295,  0.0499,  0.0543, -0.1495,  0.0168,  0.0750]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.4454,  0.0822, -0.0907, -0.1688,  0.2471, -0.1662,  0.0626, -0.0199]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2446,  0.0521, -0.0893, -0.0230,  0.0430, -0.1352,  0.0336,  0.1861]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2456, -0.1204, -0.0260,  0.0506, -0.0114, -0.1691, -0.0372, -0.0074]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.3637,  0.0364, -0.1012, -0.0764,  0.0178, -0.1150,  0.0393,  0.0031]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.3867,  0.0539, -0.0952, -0.0083,  0.0911, -0.2174,  0.0228, -0.1507]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1109,  0.0034, -0.1373,  0.0366,  0.0297, -0.0339, -0.1645,  0.1057]])\n",
      "features = torch.Size([1, 2048])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2546, -0.0750, -0.1167,  0.0242,  0.1788, -0.2048, -0.1626,  0.0873]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2699, -0.1510, -0.1471, -0.0999, -0.0456, -0.1109, -0.0793,  0.0787]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2313, -0.1679, -0.1540,  0.0869, -0.0330, -0.1436, -0.1323, -0.0398]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2957,  0.0026, -0.1652,  0.1127,  0.0153, -0.1862, -0.1453,  0.1033]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2291,  0.1291,  0.0874,  0.1756,  0.1476, -0.2508, -0.1325,  0.0505]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1494, -0.0279, -0.0867, -0.0205,  0.0396, -0.1176, -0.1182, -0.1235]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2608, -0.0919,  0.0181,  0.0407,  0.0721, -0.2508, -0.1419, -0.0278]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1973,  0.0375, -0.0190, -0.0380,  0.1933, -0.0919, -0.2543,  0.0393]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.0706,  0.0339, -0.1379,  0.0682,  0.0577, -0.0331, -0.0813,  0.0474]])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2037, -0.0533, -0.0353, -0.0345,  0.0573, -0.1194, -0.1014,  0.0680]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2488, -0.0799, -0.0561,  0.1032,  0.0234, -0.2225, -0.0882,  0.0824]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1635, -0.0165, -0.0773,  0.1882,  0.1664, -0.0246, -0.1926,  0.1002]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1563, -0.0327,  0.0200,  0.0289,  0.0990, -0.1015, -0.0407,  0.1087]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2405, -0.0253, -0.0379, -0.1264,  0.1182, -0.2441, -0.1386,  0.0873]])\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.1611, -0.0985, -0.1371,  0.0085,  0.1258, -0.1333, -0.0176,  0.0616]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1671,  0.1379, -0.2661,  ..., -0.2989,  0.1243,  0.2650],\n",
      "        [-0.1671,  0.0752,  0.0360,  ..., -0.0469,  0.0266,  0.0068],\n",
      "        [-0.2779,  0.1806, -0.1732,  ..., -0.1954, -0.1040,  0.2131],\n",
      "        ...,\n",
      "        [-0.0734, -0.0757, -0.3344,  ..., -0.2246,  0.0486,  0.3258],\n",
      "        [-0.0273,  0.0230, -0.0775,  ..., -0.2906, -0.1683, -0.0943],\n",
      "        [-0.1902,  0.0870,  0.0813,  ..., -0.2669, -0.0437,  0.1182]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-2.7502e-01, -3.5298e-02,  1.4973e-02,  ..., -1.2179e-01,\n",
      "          1.2506e-02,  4.5821e-02],\n",
      "        [-1.2232e-01,  4.3903e-02, -5.8936e-02,  ..., -1.6174e-01,\n",
      "         -9.5964e-02,  9.2891e-03],\n",
      "        [-2.4473e-01,  9.7010e-02, -1.5482e-01,  ..., -1.8219e-01,\n",
      "         -4.3125e-02,  2.1812e-01],\n",
      "        ...,\n",
      "        [-1.1112e-01,  3.0294e-02, -4.5756e-02,  ..., -4.2845e-02,\n",
      "         -5.1087e-02, -2.1669e-04],\n",
      "        [-1.0663e-01,  8.2604e-03, -2.2009e-01,  ..., -2.6803e-01,\n",
      "         -1.5840e-01,  1.7706e-03],\n",
      "        [-1.7609e-01, -1.5989e-02, -1.1877e-01,  ..., -1.4825e-01,\n",
      "          6.7097e-03, -3.3784e-02]], grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 65845.6016,  68227.0312,  66902.1328,  ...,  68297.8906,\n",
      "          68223.6250, -21882.4551],\n",
      "        [ 42137.7227,  43633.9844,  42830.4844,  ...,  43638.7812,\n",
      "          43638.0469, -13965.8652],\n",
      "        [ 58540.2812,  60652.7734,  59572.9219,  ...,  60659.5625,\n",
      "          60591.8555, -19388.5977],\n",
      "        ...,\n",
      "        [ 57535.8359,  59508.4766,  58470.3359,  ...,  59651.6328,\n",
      "          59738.1406, -18910.7344],\n",
      "        [ 18384.3008,  19062.7617,  18691.6641,  ...,  19108.7129,\n",
      "          19040.7188,  -6058.5132],\n",
      "        [107070.6094, 110837.5469, 108958.1562,  ..., 110880.7500,\n",
      "         110736.9062, -35529.0078]], grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1769, -0.0181, -0.1331,  ..., -0.0991,  0.0100, -0.0423],\n",
      "        [-0.1210,  0.0430, -0.0627,  ..., -0.1660, -0.0987,  0.0040],\n",
      "        [-0.1878, -0.0615, -0.0169,  ..., -0.3138, -0.1600,  0.0643],\n",
      "        ...,\n",
      "        [-0.0688, -0.0774, -0.3379,  ..., -0.2274,  0.0590,  0.3218],\n",
      "        [-0.2539, -0.0965, -0.1723,  ..., -0.2159, -0.1241, -0.0334],\n",
      "        [-0.1546, -0.1167, -0.2466,  ..., -0.3015, -0.0513,  0.0690]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.0345, -0.0265,  0.0671,  ...,  0.0151,  0.0488,  0.3246],\n",
      "        [-0.0657, -0.0896, -0.0025,  ..., -0.0082,  0.0103,  0.3139],\n",
      "        [-0.0177, -0.0250,  0.0153,  ...,  0.0229,  0.0485,  0.2072],\n",
      "        ...,\n",
      "        [ 0.0264,  0.0154,  0.2471,  ...,  0.2490,  0.0742,  0.9324],\n",
      "        [-0.0220, -0.0686,  0.0396,  ...,  0.0392,  0.0230,  0.4159],\n",
      "        [ 0.0336, -0.0445,  0.0172,  ...,  0.0847,  0.1168,  0.3842]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1360, -0.0619, -0.1064,  ..., -0.0646, -0.0624, -0.0563],\n",
      "        [-0.1069,  0.0213, -0.0960,  ..., -0.0400,  0.0021, -0.0503],\n",
      "        [-0.2173, -0.0017, -0.1118,  ..., -0.1947, -0.1014,  0.1019],\n",
      "        ...,\n",
      "        [-0.2929, -0.1629, -0.2476,  ..., -0.2678, -0.0913, -0.0543],\n",
      "        [-0.1212, -0.0377, -0.0965,  ..., -0.1336,  0.0055, -0.0465],\n",
      "        [-0.1746, -0.0181, -0.1257,  ..., -0.1450,  0.0032, -0.0324]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[ 3.7930,  5.1674,  2.5809,  4.9325, -4.9331,  4.0357,  2.4920,  1.5008]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 6.9723,  9.0856,  4.7263,  ...,  7.1879,  4.7027,  2.2701],\n",
      "        [ 2.2023,  3.0691,  1.6043,  ...,  2.5679,  1.3266,  0.3748],\n",
      "        [20.6009, 21.2057, 16.3038,  ..., 19.8015, 17.9553, -0.8797],\n",
      "        ...,\n",
      "        [ 3.2735,  4.4252,  2.1978,  ...,  3.5996,  2.0375,  1.0420],\n",
      "        [ 5.4075,  6.4921,  3.7141,  ...,  5.6720,  3.8334,  1.6579],\n",
      "        [11.6908, 12.3738,  8.7898,  ..., 11.3534,  9.8833,  0.0248]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-4.8092e-01, -1.5470e-01, -6.4990e-02,  ..., -1.8742e-01,\n",
      "          1.1819e-02,  1.3143e-01],\n",
      "        [-8.8139e-02, -1.1324e-01, -1.4605e-01,  ..., -2.0129e-01,\n",
      "          3.0043e-02,  1.8224e-02],\n",
      "        [ 9.2403e-03,  4.4640e-03, -1.6140e-01,  ..., -1.3588e-01,\n",
      "         -6.8900e-02,  8.5288e-02],\n",
      "        ...,\n",
      "        [-1.6190e-01, -1.1338e-03, -1.1863e-01,  ..., -1.8747e-01,\n",
      "          1.5432e-01,  2.2383e-01],\n",
      "        [-2.7674e-01,  6.2982e-02, -2.3183e-04,  ..., -8.6674e-02,\n",
      "         -1.6189e-01, -9.0718e-02],\n",
      "        [-3.2296e-01,  6.2474e-02, -7.8557e-02,  ..., -4.8328e-02,\n",
      "         -1.0552e-01,  8.0306e-02]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.7891, -1.4435, -1.4001, -1.6212,  2.4034, -0.8669, -1.5051,  4.8475]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.8018, -1.2735, -1.1441,  ..., -0.6933, -1.4764,  3.4915],\n",
      "        [-0.2615, -0.5379, -0.6596,  ..., -0.2629, -0.5842,  2.2025],\n",
      "        [-0.6174, -1.1928, -1.0986,  ..., -0.6298, -1.2193,  3.3653],\n",
      "        ...,\n",
      "        [-0.7025, -1.3949, -1.2441,  ..., -0.7579, -1.3789,  3.7275],\n",
      "        [-0.7651, -1.5538, -1.6672,  ..., -0.9290, -1.6799,  5.8695],\n",
      "        [-0.5729, -1.1870, -1.2282,  ..., -0.6903, -1.2526,  3.9292]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.0204,  0.0527, -0.1590,  ..., -0.1919, -0.0850,  0.0890],\n",
      "        [-0.0878,  0.0548, -0.1057,  ..., -0.0266,  0.0041, -0.0308],\n",
      "        [-0.1743,  0.1022, -0.2958,  ..., -0.3163,  0.1172,  0.2672],\n",
      "        ...,\n",
      "        [-0.2662, -0.1210, -0.0732,  ..., -0.2478, -0.1836,  0.0200],\n",
      "        [-0.1622, -0.0206, -0.1290,  ..., -0.0934, -0.0008, -0.0433],\n",
      "        [-0.2094, -0.0036, -0.1390,  ..., -0.2218,  0.0466, -0.0414]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-6.4568e-02,  1.0480e-01,  1.5423e-03,  ...,  1.3410e-01,\n",
      "          3.7060e-03,  1.7577e-01],\n",
      "        [-4.7962e-02,  6.1152e-02, -6.9129e-03,  ...,  5.4684e-02,\n",
      "          1.0245e-03,  3.1263e-02],\n",
      "        [ 9.9005e-02,  2.8602e-01,  3.4240e-02,  ...,  2.1258e-01,\n",
      "          9.5642e-02, -2.2296e-01],\n",
      "        ...,\n",
      "        [ 1.8984e-01,  4.5261e-01,  1.0506e-01,  ...,  3.5582e-01,\n",
      "          1.1776e-01,  5.0011e-03],\n",
      "        [ 1.1988e+00,  1.8023e+00,  4.6187e-01,  ...,  1.5248e+00,\n",
      "          5.6041e-01, -9.0434e-01],\n",
      "        [-2.2935e-02,  7.6104e-02,  2.0423e-03,  ...,  5.4936e-02,\n",
      "          2.2733e-02,  5.2542e-03]], grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2361,  0.0154, -0.0785,  ..., -0.2029,  0.0973,  0.2761],\n",
      "        [-0.1428, -0.0973, -0.1688,  ..., -0.1370, -0.1021,  0.0943],\n",
      "        [-0.2550, -0.0782, -0.1749,  ..., -0.2159, -0.1298, -0.0366],\n",
      "        ...,\n",
      "        [-0.4596, -0.0248, -0.0422,  ..., -0.3862, -0.0216,  0.1691],\n",
      "        [-0.4173, -0.1196, -0.0575,  ..., -0.2128, -0.0342,  0.1095],\n",
      "        [-0.3018,  0.1328, -0.1334,  ..., -0.4034, -0.1815,  0.1525]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.0036,  0.0863,  0.0359,  ...,  0.0592,  0.0289,  0.1788],\n",
      "        [-0.0036,  0.0863,  0.0359,  ...,  0.0592,  0.0289,  0.1788],\n",
      "        [-0.0036,  0.0863,  0.0359,  ...,  0.0592,  0.0289,  0.1788],\n",
      "        ...,\n",
      "        [-0.0036,  0.0863,  0.0359,  ...,  0.0592,  0.0289,  0.1788],\n",
      "        [-0.0119,  0.1220,  0.0899,  ...,  0.1156,  0.0224,  0.0015],\n",
      "        [-0.0036,  0.0863,  0.0359,  ...,  0.0592,  0.0289,  0.1788]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.0654, -0.0135, -0.0541,  ..., -0.3171, -0.0559,  0.0194],\n",
      "        [-0.1573,  0.0101, -0.1884,  ..., -0.0527, -0.0230,  0.0097],\n",
      "        [-0.0793,  0.0533, -0.1681,  ..., -0.2615, -0.1499,  0.0993],\n",
      "        ...,\n",
      "        [-0.1152, -0.0554, -0.0962,  ..., -0.1490,  0.0071, -0.0540],\n",
      "        [-0.2610, -0.1085, -0.1815,  ..., -0.2527, -0.1599, -0.0439],\n",
      "        [-0.0975,  0.0316, -0.1130,  ..., -0.0252,  0.0034, -0.0382]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.0120,  0.0729,  0.0171,  ...,  0.0694,  0.0326,  0.0084],\n",
      "        [-0.0120,  0.0729,  0.0171,  ...,  0.0694,  0.0326,  0.0084],\n",
      "        [-0.0740,  0.0919,  0.0306,  ...,  0.1665, -0.0014,  0.1695],\n",
      "        ...,\n",
      "        [-0.0120,  0.0729,  0.0171,  ...,  0.0694,  0.0326,  0.0084],\n",
      "        [-0.0120,  0.0729,  0.0171,  ...,  0.0694,  0.0326,  0.0084],\n",
      "        [-0.0120,  0.0729,  0.0171,  ...,  0.0694,  0.0326,  0.0084]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2843, -0.0654, -0.1192,  ...,  0.0077, -0.0651,  0.0786],\n",
      "        [-0.2461,  0.0301, -0.0314,  ..., -0.0988,  0.0482,  0.1586],\n",
      "        [-0.2198,  0.0247, -0.0632,  ..., -0.2107,  0.0515,  0.2826],\n",
      "        ...,\n",
      "        [-0.2635,  0.1238, -0.1756,  ..., -0.2419,  0.0868,  0.0797],\n",
      "        [-0.1555,  0.0126, -0.0832,  ..., -0.0707,  0.0162, -0.0517],\n",
      "        [-0.2528, -0.1591, -0.0991,  ..., -0.2725, -0.1049, -0.0126]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[ 0.0545,  0.1459,  0.0795,  0.0727, -0.0368,  0.1497,  0.0940,  0.0525]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0545, 0.1459, 0.0795,  ..., 0.1497, 0.0940, 0.0525],\n",
      "        [0.0545, 0.1459, 0.0795,  ..., 0.1497, 0.0940, 0.0525],\n",
      "        [0.0545, 0.1459, 0.0795,  ..., 0.1497, 0.0940, 0.0525],\n",
      "        ...,\n",
      "        [0.0545, 0.1459, 0.0795,  ..., 0.1497, 0.0940, 0.0525],\n",
      "        [0.0545, 0.1459, 0.0795,  ..., 0.1497, 0.0940, 0.0525],\n",
      "        [0.0545, 0.1459, 0.0795,  ..., 0.1497, 0.0940, 0.0525]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2126, -0.0276, -0.0741,  ..., -0.1199,  0.0163, -0.0092],\n",
      "        [-0.1619, -0.0791, -0.0345,  ..., -0.1396,  0.0032,  0.0587],\n",
      "        [-0.1877, -0.0527, -0.1003,  ..., -0.1172, -0.0172, -0.0332],\n",
      "        ...,\n",
      "        [-0.1305, -0.0449, -0.0180,  ...,  0.0016, -0.0331,  0.1468],\n",
      "        [-0.1795,  0.0749, -0.1127,  ..., -0.2071, -0.1126, -0.0738],\n",
      "        [-0.1677, -0.0236, -0.1013,  ..., -0.0968, -0.0107, -0.0335]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0010, 0.0887, 0.0280,  ..., 0.1149, 0.0585, 0.0654],\n",
      "        [0.0741, 0.0862, 0.0849,  ..., 0.2413, 0.1878, 0.2342],\n",
      "        [0.0010, 0.0887, 0.0280,  ..., 0.1149, 0.0585, 0.0654],\n",
      "        ...,\n",
      "        [0.0010, 0.0887, 0.0280,  ..., 0.1149, 0.0585, 0.0654],\n",
      "        [0.0010, 0.0887, 0.0280,  ..., 0.1149, 0.0585, 0.0654],\n",
      "        [0.0010, 0.0887, 0.0280,  ..., 0.1149, 0.0585, 0.0654]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2520, -0.0515, -0.0293,  ..., -0.1994, -0.1298,  0.1449],\n",
      "        [-0.1796,  0.0178, -0.1163,  ..., -0.2163,  0.1224,  0.2173],\n",
      "        [-0.0102,  0.0339, -0.1499,  ..., -0.2534, -0.0789,  0.0304],\n",
      "        ...,\n",
      "        [-0.0286, -0.0799, -0.1853,  ..., -0.3234, -0.2743,  0.0654],\n",
      "        [-0.2187,  0.0387, -0.1498,  ..., -0.2901, -0.1196,  0.0902],\n",
      "        [-0.3164,  0.0898, -0.0770,  ..., -0.0581, -0.1173,  0.0850]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[0.0182, 0.1164, 0.0429, 0.0343, 0.0047, 0.1705, 0.0851, 0.0641]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.0182,  0.1164,  0.0429,  ...,  0.1705,  0.0851,  0.0641],\n",
      "        [ 0.1953,  0.4491,  0.3606,  ...,  0.7978,  0.2867, -0.1539],\n",
      "        [ 0.0182,  0.1164,  0.0429,  ...,  0.1705,  0.0851,  0.0641],\n",
      "        ...,\n",
      "        [ 0.0182,  0.1164,  0.0429,  ...,  0.1705,  0.0851,  0.0641],\n",
      "        [ 0.0182,  0.1164,  0.0429,  ...,  0.1705,  0.0851,  0.0641],\n",
      "        [ 0.0182,  0.1164,  0.0429,  ...,  0.1705,  0.0851,  0.0641]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1398,  0.0636, -0.0551,  ..., -0.1872, -0.0545, -0.0017],\n",
      "        [-0.1664,  0.0268, -0.0803,  ..., -0.0869,  0.0032, -0.0350],\n",
      "        [-0.0781,  0.0854, -0.1181,  ..., -0.2564, -0.0223,  0.1103],\n",
      "        ...,\n",
      "        [-0.2515,  0.1483,  0.0875,  ..., -0.3642, -0.1111,  0.1697],\n",
      "        [-0.0066, -0.0359, -0.1682,  ..., -0.3071, -0.2676,  0.0676],\n",
      "        [-0.1587, -0.0480, -0.1873,  ..., -0.2170,  0.0363,  0.0607]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[-0.2705, -0.0653, -0.1215, -0.3140,  0.2179,  0.0890, -0.1059,  2.4759]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-1.1608e-03,  9.6646e-02,  5.1680e-02,  ...,  1.6984e-01,\n",
      "          6.2534e-02,  5.3170e-02],\n",
      "        [-1.1693e+01, -1.8868e+00, -7.3673e+00,  ...,  3.3784e+00,\n",
      "         -5.4400e+00,  1.1834e+02],\n",
      "        [-1.1608e-03,  9.6646e-02,  5.1680e-02,  ...,  1.6984e-01,\n",
      "          6.2534e-02,  5.3170e-02],\n",
      "        ...,\n",
      "        [-1.1608e-03,  9.6646e-02,  5.1680e-02,  ...,  1.6984e-01,\n",
      "          6.2534e-02,  5.3170e-02],\n",
      "        [-1.1608e-03,  9.6646e-02,  5.1680e-02,  ...,  1.6984e-01,\n",
      "          6.2534e-02,  5.3170e-02],\n",
      "        [-1.1608e-03,  9.6646e-02,  5.1680e-02,  ...,  1.6984e-01,\n",
      "          6.2534e-02,  5.3170e-02]], grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.4101, -0.0433, -0.1218,  ..., -0.3357, -0.0809,  0.1685],\n",
      "        [-0.2624, -0.0125, -0.0951,  ..., -0.3696, -0.0552,  0.1698],\n",
      "        [-0.1574, -0.0586, -0.0900,  ..., -0.0703, -0.0835, -0.0120],\n",
      "        ...,\n",
      "        [-0.2148, -0.1851, -0.0817,  ..., -0.3916, -0.1786,  0.2337],\n",
      "        [-0.1280, -0.0067, -0.2385,  ..., -0.3075, -0.1646,  0.0453],\n",
      "        [-0.2331, -0.0124, -0.0228,  ..., -0.0121, -0.1327,  0.0007]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.1405,  0.2074,  0.1853,  ...,  0.1649,  0.1909, -0.2197],\n",
      "        [ 0.1415,  0.2098,  0.1866,  ...,  0.1674,  0.1912, -0.2212],\n",
      "        [ 0.1415,  0.2098,  0.1866,  ...,  0.1674,  0.1912, -0.2212],\n",
      "        ...,\n",
      "        [ 0.1415,  0.2098,  0.1866,  ...,  0.1674,  0.1912, -0.2212],\n",
      "        [ 0.1415,  0.2098,  0.1866,  ...,  0.1674,  0.1912, -0.2212],\n",
      "        [ 0.1415,  0.2098,  0.1866,  ...,  0.1674,  0.1912, -0.2212]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2123,  0.1175,  0.1685,  ..., -0.4076, -0.1136,  0.1466],\n",
      "        [-0.2760, -0.0513, -0.0480,  ..., -0.2093, -0.1242,  0.1049],\n",
      "        [-0.1932,  0.0241, -0.1354,  ..., -0.2773, -0.0851,  0.0542],\n",
      "        ...,\n",
      "        [-0.1283, -0.0366, -0.0082,  ..., -0.1101,  0.0460,  0.0596],\n",
      "        [-0.1286,  0.0399, -0.1096,  ..., -0.1182,  0.0685, -0.0093],\n",
      "        [-0.1141,  0.0592, -0.0742,  ..., -0.2482, -0.1198,  0.1019]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[ 0.0338,  0.1069,  0.0899,  0.0420, -0.0170,  0.0884,  0.0865, -0.0539]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.0338,  0.1069,  0.0899,  ...,  0.0884,  0.0865, -0.0539],\n",
      "        [ 0.0338,  0.1069,  0.0899,  ...,  0.0884,  0.0865, -0.0539],\n",
      "        [ 0.0338,  0.1069,  0.0899,  ...,  0.0884,  0.0865, -0.0539],\n",
      "        ...,\n",
      "        [ 0.0338,  0.1069,  0.0899,  ...,  0.0884,  0.0865, -0.0539],\n",
      "        [ 0.0338,  0.1069,  0.0899,  ...,  0.0884,  0.0865, -0.0539],\n",
      "        [ 0.0338,  0.1069,  0.0899,  ...,  0.0884,  0.0865, -0.0539]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1484,  0.0064, -0.2271,  ..., -0.3143, -0.1775,  0.0352],\n",
      "        [-0.3999, -0.0324, -0.1056,  ..., -0.3247, -0.1098,  0.2155],\n",
      "        [-0.2503, -0.0620, -0.0195,  ..., -0.1936, -0.1268,  0.1277],\n",
      "        ...,\n",
      "        [-0.0927, -0.0303, -0.1902,  ..., -0.0669, -0.3210,  0.0171],\n",
      "        [-0.2203, -0.0053, -0.0259,  ..., -0.2008, -0.0049, -0.0018],\n",
      "        [-0.1696,  0.0818, -0.0394,  ..., -0.1462, -0.0853, -0.0040]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[0.0294, 0.1167, 0.0910, 0.0423, 0.0156, 0.1555, 0.0963, 0.0003]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.0294,  0.1167,  0.0910,  ...,  0.1555,  0.0963,  0.0003],\n",
      "        [ 0.0210,  0.1039,  0.0825,  ...,  0.1274,  0.0795, -0.0005],\n",
      "        [ 0.0294,  0.1167,  0.0910,  ...,  0.1555,  0.0963,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0294,  0.1167,  0.0910,  ...,  0.1555,  0.0963,  0.0003],\n",
      "        [ 0.0294,  0.1167,  0.0910,  ...,  0.1555,  0.0963,  0.0003],\n",
      "        [ 0.0294,  0.1167,  0.0910,  ...,  0.1555,  0.0963,  0.0003]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.8268,  0.0624, -0.2190,  ..., -0.3822, -0.4300, -0.1717],\n",
      "        [-0.1176, -0.0395, -0.2368,  ..., -0.0799, -0.2722, -0.0114],\n",
      "        [-0.1206, -0.0943, -0.0746,  ..., -0.1644,  0.0094,  0.0291],\n",
      "        ...,\n",
      "        [-0.3783,  0.0296, -0.1242,  ..., -0.3967, -0.0025, -0.1816],\n",
      "        [-0.3229,  0.0012, -0.2564,  ..., -0.1633, -0.0234,  0.0288],\n",
      "        [-0.1848, -0.0011, -0.1391,  ..., -0.1974,  0.0405, -0.0450]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[0.0300, 0.1346, 0.0849, 0.0210, 0.0260, 0.2045, 0.0975, 0.0279]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.0300,  0.1346,  0.0849,  ...,  0.2045,  0.0975,  0.0279],\n",
      "        [-0.0394,  0.0457,  0.0575,  ...,  0.0050,  0.0261,  0.1049],\n",
      "        [ 0.0300,  0.1346,  0.0849,  ...,  0.2045,  0.0975,  0.0279],\n",
      "        ...,\n",
      "        [ 0.0300,  0.1346,  0.0849,  ...,  0.2045,  0.0975,  0.0279],\n",
      "        [ 0.0300,  0.1346,  0.0849,  ...,  0.2045,  0.0975,  0.0279],\n",
      "        [ 0.0300,  0.1346,  0.0849,  ...,  0.2045,  0.0975,  0.0279]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.3121,  0.0235, -0.2351,  ..., -0.2109, -0.0018,  0.0684],\n",
      "        [-0.2806, -0.0358, -0.1214,  ..., -0.3712, -0.0505,  0.1941],\n",
      "        [-0.2654,  0.0811, -0.0227,  ..., -0.1259, -0.1088, -0.0485],\n",
      "        ...,\n",
      "        [-0.1415,  0.0547, -0.1011,  ..., -0.2646, -0.0667,  0.1418],\n",
      "        [-0.2104, -0.0295, -0.0632,  ..., -0.1264,  0.0204, -0.0136],\n",
      "        [-0.2362, -0.1447, -0.1019,  ..., -0.2685, -0.1283, -0.0151]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.0287,  0.1067,  0.0652,  ...,  0.1430,  0.0744,  0.0276],\n",
      "        [ 0.0287,  0.1067,  0.0652,  ...,  0.1430,  0.0744,  0.0276],\n",
      "        [-0.0087,  0.0431,  0.0496,  ...,  0.0690,  0.0469,  0.0172],\n",
      "        ...,\n",
      "        [ 0.0287,  0.1067,  0.0652,  ...,  0.1430,  0.0744,  0.0276],\n",
      "        [ 0.0697,  1.0759,  0.8976,  ...,  3.2698,  0.4348, -0.2195],\n",
      "        [ 0.0287,  0.1067,  0.0652,  ...,  0.1430,  0.0744,  0.0276]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2232,  0.0026, -0.0440,  ..., -0.0079, -0.1351,  0.0092],\n",
      "        [-0.2588, -0.1223, -0.1864,  ..., -0.1570,  0.0934, -0.0619],\n",
      "        [-0.1181,  0.0541, -0.0682,  ..., -0.0459, -0.0713,  0.0104],\n",
      "        ...,\n",
      "        [-0.2396, -0.0075, -0.0376,  ..., -0.0169, -0.0144, -0.0434],\n",
      "        [-0.1156,  0.0187, -0.0905,  ..., -0.0456,  0.0116, -0.0511],\n",
      "        [-0.1301,  0.0437, -0.0458,  ..., -0.0923, -0.1101, -0.0195]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.0321,  0.1188,  0.0649,  ...,  0.2075,  0.0852,  0.0510],\n",
      "        [ 0.0321,  0.1188,  0.0649,  ...,  0.2075,  0.0852,  0.0510],\n",
      "        [ 0.0321,  0.1188,  0.0649,  ...,  0.2075,  0.0852,  0.0510],\n",
      "        ...,\n",
      "        [ 0.1389,  2.4581,  2.0425,  ...,  8.3620,  0.9068, -0.2483],\n",
      "        [ 0.0321,  0.1188,  0.0649,  ...,  0.2075,  0.0852,  0.0510],\n",
      "        [ 0.0321,  0.1188,  0.0649,  ...,  0.2075,  0.0852,  0.0510]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.5671, -0.1739, -0.4028,  ..., -0.2114,  0.0446,  0.0927],\n",
      "        [-0.2658,  0.0085, -0.2173,  ..., -0.1484, -0.0031,  0.0439],\n",
      "        [-0.2636,  0.0959, -0.1093,  ..., -0.2102, -0.3322,  0.0369],\n",
      "        ...,\n",
      "        [-0.1809,  0.0211, -0.0609,  ..., -0.0983,  0.0099, -0.0294],\n",
      "        [-0.1074,  0.0419, -0.0830,  ..., -0.0058,  0.0048, -0.0310],\n",
      "        [-0.2620,  0.0694, -0.1970,  ..., -0.2419,  0.1271,  0.1266]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[ 0.0010,  0.0742,  0.0538,  0.0002, -0.0055,  0.0419,  0.0450,  0.0267]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0010, 0.0742, 0.0538,  ..., 0.0419, 0.0450, 0.0267],\n",
      "        [0.0010, 0.0742, 0.0538,  ..., 0.0419, 0.0450, 0.0267],\n",
      "        [0.0010, 0.0742, 0.0538,  ..., 0.0419, 0.0450, 0.0267],\n",
      "        ...,\n",
      "        [0.0010, 0.0742, 0.0538,  ..., 0.0419, 0.0450, 0.0267],\n",
      "        [0.0010, 0.0742, 0.0538,  ..., 0.0419, 0.0450, 0.0267],\n",
      "        [0.0010, 0.0742, 0.0538,  ..., 0.0419, 0.0450, 0.0267]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1090, -0.0028, -0.0959,  ..., -0.0748, -0.0319, -0.0405],\n",
      "        [-0.1376, -0.0529, -0.0537,  ..., -0.1616,  0.0174,  0.0185],\n",
      "        [-0.2790, -0.0262, -0.0277,  ..., -0.0888, -0.1159, -0.1893],\n",
      "        ...,\n",
      "        [-0.2191,  0.0066, -0.1432,  ..., -0.2392,  0.0325, -0.0426],\n",
      "        [-0.0899,  0.0288, -0.1098,  ..., -0.0096, -0.0061, -0.0080],\n",
      "        [-0.4805, -0.0515, -0.1131,  ..., -0.3737, -0.0643,  0.0686]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[ 0.0150,  0.0963,  0.0651,  ...,  0.0709,  0.0561,  0.0397],\n",
      "        [ 0.0150,  0.0963,  0.0651,  ...,  0.0709,  0.0561,  0.0397],\n",
      "        [ 0.0150,  0.0963,  0.0651,  ...,  0.0709,  0.0561,  0.0397],\n",
      "        ...,\n",
      "        [-0.0508,  0.0673,  0.1011,  ..., -0.0006,  0.0087,  0.2498],\n",
      "        [ 0.0150,  0.0963,  0.0651,  ...,  0.0709,  0.0561,  0.0397],\n",
      "        [ 0.0150,  0.0963,  0.0651,  ...,  0.0709,  0.0561,  0.0397]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1646, -0.0086, -0.1099,  ..., -0.1404,  0.0086, -0.0393],\n",
      "        [-0.0967,  0.0261, -0.0998,  ..., -0.0260, -0.0008, -0.0366],\n",
      "        [-0.1527,  0.0121, -0.0057,  ..., -0.1970, -0.0426,  0.1914],\n",
      "        ...,\n",
      "        [-0.2882,  0.0313, -0.0867,  ..., -0.3443, -0.0337,  0.1500],\n",
      "        [-0.2471, -0.0471, -0.0576,  ..., -0.1930, -0.1009,  0.0979],\n",
      "        [-0.2547, -0.0206, -0.0383,  ..., -0.1901,  0.0139, -0.0883]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0254, 0.1127, 0.0703,  ..., 0.1114, 0.0629, 0.0516],\n",
      "        [0.0254, 0.1127, 0.0703,  ..., 0.1114, 0.0629, 0.0516],\n",
      "        [0.0254, 0.1127, 0.0703,  ..., 0.1114, 0.0629, 0.0516],\n",
      "        ...,\n",
      "        [0.0254, 0.1127, 0.0703,  ..., 0.1114, 0.0629, 0.0516],\n",
      "        [0.0254, 0.1127, 0.0703,  ..., 0.1114, 0.0629, 0.0516],\n",
      "        [0.0254, 0.1127, 0.0703,  ..., 0.1114, 0.0629, 0.0516]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1719,  0.0150, -0.0675,  ..., -0.0896,  0.0056, -0.0367],\n",
      "        [-0.0647,  0.0879,  0.0657,  ..., -0.2271, -0.0678,  0.0383],\n",
      "        [-0.1383,  0.0077, -0.0208,  ..., -0.3561, -0.0809,  0.0464],\n",
      "        ...,\n",
      "        [-0.1756, -0.0597, -0.0182,  ..., -0.2779, -0.1665,  0.0370],\n",
      "        [-0.2277, -0.0045, -0.0518,  ..., -0.1789,  0.0293, -0.0909],\n",
      "        [-0.0736,  0.0747, -0.1153,  ..., -0.2664, -0.0311,  0.0640]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0314, 0.1143, 0.0753,  ..., 0.1364, 0.0674, 0.0579],\n",
      "        [0.0314, 0.1143, 0.0753,  ..., 0.1364, 0.0674, 0.0579],\n",
      "        [0.0314, 0.1143, 0.0753,  ..., 0.1364, 0.0674, 0.0579],\n",
      "        ...,\n",
      "        [0.0314, 0.1143, 0.0753,  ..., 0.1364, 0.0674, 0.0579],\n",
      "        [0.0314, 0.1143, 0.0753,  ..., 0.1364, 0.0674, 0.0579],\n",
      "        [0.0314, 0.1143, 0.0753,  ..., 0.1364, 0.0674, 0.0579]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2739,  0.0588,  0.0644,  ..., -0.1220, -0.2114, -0.0951],\n",
      "        [-0.1946, -0.0500, -0.1396,  ..., -0.2322,  0.0174,  0.0554],\n",
      "        [-0.1913,  0.0712,  0.0645,  ..., -0.0664,  0.0121,  0.0288],\n",
      "        ...,\n",
      "        [-0.1688,  0.0332, -0.1863,  ..., -0.0638, -0.0331,  0.0057],\n",
      "        [-0.2724, -0.0113, -0.0667,  ..., -0.0214, -0.0083, -0.0235],\n",
      "        [-0.0926,  0.0297, -0.0811,  ..., -0.0080, -0.0084, -0.0359]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[0.0295, 0.0916, 0.0719, 0.0327, 0.0219, 0.1384, 0.0688, 0.0557]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0295, 0.0916, 0.0719,  ..., 0.1384, 0.0688, 0.0557],\n",
      "        [0.0295, 0.0916, 0.0719,  ..., 0.1384, 0.0688, 0.0557],\n",
      "        [0.0295, 0.0916, 0.0719,  ..., 0.1384, 0.0688, 0.0557],\n",
      "        ...,\n",
      "        [0.0295, 0.0916, 0.0719,  ..., 0.1384, 0.0688, 0.0557],\n",
      "        [0.0295, 0.0916, 0.0719,  ..., 0.1384, 0.0688, 0.0557],\n",
      "        [0.0295, 0.0916, 0.0719,  ..., 0.1384, 0.0688, 0.0557]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.2135,  0.0091, -0.1038,  ..., -0.1822,  0.0097,  0.0700],\n",
      "        [-0.2922, -0.0718,  0.0014,  ..., -0.1897, -0.1899,  0.0415],\n",
      "        [-0.2455, -0.0865, -0.1237,  ...,  0.0204, -0.1133,  0.1031],\n",
      "        ...,\n",
      "        [ 0.0198,  0.0220, -0.1650,  ..., -0.1363, -0.0960,  0.0801],\n",
      "        [-0.1355, -0.0726, -0.0912,  ..., -0.1625,  0.0276,  0.0185],\n",
      "        [-0.1355,  0.0277, -0.0341,  ..., -0.1141, -0.0639,  0.0500]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0357, 0.1151, 0.0737,  ..., 0.1521, 0.0754, 0.0559],\n",
      "        [0.0357, 0.1151, 0.0737,  ..., 0.1521, 0.0754, 0.0559],\n",
      "        [0.0357, 0.1151, 0.0737,  ..., 0.1521, 0.0754, 0.0559],\n",
      "        ...,\n",
      "        [0.1814, 0.3696, 0.4240,  ..., 0.9707, 0.3712, 0.1309],\n",
      "        [0.0357, 0.1151, 0.0737,  ..., 0.1521, 0.0754, 0.0559],\n",
      "        [0.0357, 0.1151, 0.0737,  ..., 0.1521, 0.0754, 0.0559]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1201, -0.0542, -0.1138,  ..., -0.1526,  0.0157, -0.0497],\n",
      "        [-0.2016, -0.0384, -0.1712,  ..., -0.1463, -0.1749, -0.0529],\n",
      "        [ 0.0149,  0.0248, -0.1399,  ..., -0.1575, -0.0901,  0.0984],\n",
      "        ...,\n",
      "        [-0.5068, -0.1935, -0.0387,  ..., -0.4913, -0.3305,  0.4929],\n",
      "        [-0.2356, -0.0347, -0.0309,  ..., -0.2100, -0.1966,  0.1823],\n",
      "        [-0.2737, -0.0233, -0.0402,  ..., -0.1958,  0.0124, -0.0957]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[0.0374, 0.1250, 0.0647, 0.0385, 0.0153, 0.1539, 0.0768, 0.0548]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0374, 0.1250, 0.0647,  ..., 0.1539, 0.0768, 0.0548],\n",
      "        [0.0374, 0.1250, 0.0647,  ..., 0.1539, 0.0768, 0.0548],\n",
      "        [0.0374, 0.1250, 0.0647,  ..., 0.1539, 0.0768, 0.0548],\n",
      "        ...,\n",
      "        [0.0374, 0.1250, 0.0647,  ..., 0.1539, 0.0768, 0.0548],\n",
      "        [0.0374, 0.1250, 0.0647,  ..., 0.1539, 0.0768, 0.0548],\n",
      "        [0.0374, 0.1250, 0.0647,  ..., 0.1539, 0.0768, 0.0548]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.3089, -0.0328, -0.0437,  ..., -0.2575, -0.1399,  0.0420],\n",
      "        [-0.3040, -0.0058, -0.2354,  ..., -0.2105, -0.0516,  0.1334],\n",
      "        [-0.1470,  0.0674, -0.0065,  ..., -0.1407, -0.0506, -0.0105],\n",
      "        ...,\n",
      "        [-0.2576,  0.0180, -0.0503,  ..., -0.1797, -0.0800,  0.0950],\n",
      "        [-0.1565,  0.0059, -0.0383,  ..., -0.2790, -0.0818,  0.0853],\n",
      "        [-0.2652, -0.0327, -0.2015,  ..., -0.2756, -0.1101,  0.0290]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0330, 0.1209, 0.0729,  ..., 0.1411, 0.0721, 0.0358],\n",
      "        [0.0330, 0.1209, 0.0729,  ..., 0.1411, 0.0721, 0.0358],\n",
      "        [0.0330, 0.1209, 0.0729,  ..., 0.1411, 0.0721, 0.0358],\n",
      "        ...,\n",
      "        [0.0207, 0.1059, 0.0531,  ..., 0.0692, 0.0779, 0.0071],\n",
      "        [0.0330, 0.1209, 0.0729,  ..., 0.1411, 0.0721, 0.0358],\n",
      "        [0.0330, 0.1209, 0.0729,  ..., 0.1411, 0.0721, 0.0358]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.1413, -0.0631, -0.3467,  ..., -0.3435, -0.0568,  0.0703],\n",
      "        [-0.0547,  0.0904, -0.1935,  ..., -0.2863, -0.1285,  0.0895],\n",
      "        [-0.1461,  0.0558, -0.0826,  ..., -0.1395, -0.0806, -0.0482],\n",
      "        ...,\n",
      "        [-0.1189, -0.0308, -0.0708,  ..., -0.1665,  0.0529, -0.0990],\n",
      "        [-0.3071, -0.0497, -0.1089,  ..., -0.2057, -0.0383, -0.0488],\n",
      "        [-0.1057,  0.0480, -0.0939,  ..., -0.0107,  0.0119, -0.0430]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n",
      "dense1.shape =  torch.Size([1, 1024])\n",
      "dense2.shape =  torch.Size([1, 512])\n",
      "output =  tensor([[0.0311, 0.1124, 0.0657, 0.0378, 0.0076, 0.1231, 0.0660, 0.0406]])\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[0.0311, 0.1124, 0.0657,  ..., 0.1231, 0.0660, 0.0406],\n",
      "        [0.0311, 0.1124, 0.0657,  ..., 0.1231, 0.0660, 0.0406],\n",
      "        [0.0311, 0.1124, 0.0657,  ..., 0.1231, 0.0660, 0.0406],\n",
      "        ...,\n",
      "        [0.0311, 0.1124, 0.0657,  ..., 0.1231, 0.0660, 0.0406],\n",
      "        [0.0311, 0.1124, 0.0657,  ..., 0.1231, 0.0660, 0.0406],\n",
      "        [0.0311, 0.1124, 0.0657,  ..., 0.1231, 0.0660, 0.0406]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "dense1.shape =  torch.Size([128, 1024])\n",
      "dense2.shape =  torch.Size([128, 512])\n",
      "output =  tensor([[-0.0766, -0.0199, -0.1392,  ..., -0.2233, -0.2156,  0.0359],\n",
      "        [-0.1639, -0.0701, -0.0615,  ..., -0.2869, -0.1894,  0.0843],\n",
      "        [-0.1527, -0.0254, -0.1416,  ..., -0.1837, -0.0590,  0.0392],\n",
      "        ...,\n",
      "        [-0.3077, -0.1326, -0.2157,  ..., -0.3226, -0.1114, -0.0395],\n",
      "        [-0.0813, -0.0054,  0.0177,  ..., -0.0503, -0.1041,  0.1550],\n",
      "        [-0.1982,  0.1296,  0.0568,  ..., -0.2506, -0.2386,  0.0552]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "features = torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    ep, rewards = model.train(\n",
    "        100, render=False, epoch=i)\n",
    "    model.save(\"rl_progress_ep_\" + str(i * 50))\n",
    "    model.generate_policy_video(\"rl_progress_ep_\" + str(i*50))\n",
    "plt.savefig(\"rl_progress_plt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
