{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pabou\\miniconda3\\envs\\824project\\lib\\site-packages\\torchvision\\transforms\\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import gc\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import imageio\n",
    "import base64\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "from torchsummary import summary ######## pip install torch-summary ######## DELETE ########\n",
    "\n",
    "\n",
    "def memory_used():\n",
    "    return psutil.Process(os.getpid()).memory_info().rss * 1e-6  # To megabyte\n",
    "\n",
    "'''\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "'''\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MEMORY_CAPACITY = 7000\n",
    "NUM_TRAINING_EPISODES = 50\n",
    "MAX_EPISODE_TIME = 1000\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "ENV_CLEAR = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 15, 10])\n"
     ]
    }
   ],
   "source": [
    "######### RESNET #########\n",
    "# SOURCE: https://medium.com/the-owl/extracting-features-from-an-intermediate-layer-of-a-pretrained-model-in-pytorch-c00589bda32b\n",
    "\n",
    "#rn18 = models.resnet18(pretrained=True) ##### Expects 4-dimensional input for 4-dimensional weight [64, 3, 7, 7]\n",
    "#children_counter = 0\n",
    "#for n,c in rn18.named_children():\n",
    "#    print(\"Children Counter: \",children_counter,\" Layer Name: \",n,)\n",
    "#    children_counter+=1\n",
    "\n",
    "class rn18_feature_extractor(nn.Module):\n",
    "    def __init__(self,output_layer = None):\n",
    "        super().__init__()\n",
    "        self.pretrained = models.resnet18(pretrained=True)\n",
    "        self.output_layer = output_layer\n",
    "        self.layers = list(self.pretrained._modules.keys())\n",
    "        self.layer_count = 0\n",
    "        for l in self.layers:\n",
    "            if l != self.output_layer:\n",
    "                self.layer_count += 1\n",
    "            else:\n",
    "                break\n",
    "        for i in range(1,len(self.layers)-self.layer_count):\n",
    "            self.dummy_var = self.pretrained._modules.pop(self.layers[-i])\n",
    "        self.dummy_var= None ##### freeing up some space\n",
    "        \n",
    "        self.net = nn.Sequential(self.pretrained._modules)\n",
    "        self.pretrained = None ##### freeing up some space\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "rn18_fe = rn18_feature_extractor(output_layer='maxpool')\n",
    "\n",
    "img = torch.Tensor(1, 3, 60, 40) ##### returns torch.Size([1, 64, 15, 10])\n",
    "features = rn18_fe(img)\n",
    "print(features.shape)\n",
    "#rn18._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       9,408\n",
      "|    └─BatchNorm2d: 2-2                  128\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─MaxPool2d: 2-4                    --\n",
      "=================================================================\n",
      "Total params: 9,536\n",
      "Trainable params: 9,536\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "********************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pretrained', None),\n",
       "             ('dummy_var', None),\n",
       "             ('net', Sequential(\n",
       "                (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "                (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (relu): ReLU(inplace=True)\n",
       "                (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "              ))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(rn18_fe, input_size=(1, 3, 60, 40))\n",
    "print('*'*80)\n",
    "rn18_fe._modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        \"\"\"\n",
    "            :param h is the height of the image (in this case 60)\n",
    "            :param w is the height of the image (in this case 40)\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        #self.bn1 = nn.BatchNorm2d(16)\n",
    "        #self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(32)\n",
    "        #self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        #self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        \n",
    "        ##### DELETE #####\n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        \n",
    "        #--------------- USING BASIC ARCHITECTURE ---------------\n",
    "        #self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=2)\n",
    "        #self.bn1 = nn.BatchNorm2d(64)\n",
    "        #self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(128)\n",
    "        #self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2)\n",
    "        #self.bn3 = nn.BatchNorm2d(256)\n",
    "        #linear_input_size = conv2d_size_out(conv2d_size_out(conv2d_size_out(w))) * conv2d_size_out(conv2d_size_out(conv2d_size_out(h))) * 256\n",
    "        #print(linear_input_size)\n",
    "        #self.dense1 = nn.Linear(linear_input_size, 1024)\n",
    "        #self.dense2 = nn.Linear(1024, 512)\n",
    "        #self.head = nn.Linear(512, outputs)\n",
    "        \n",
    "        \n",
    "        #--------------- USING PRE-TRAINED RESNET18 ---------------\n",
    "        self.rn18_fe = rn18_feature_extractor(output_layer='maxpool')\n",
    "        self.conv1 = nn.Conv2d(64, 128, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        #self.conv3 = nn.Conv2d(256, 512, kernel_size=5, stride=1)\n",
    "        linear_input_size = conv2d_size_out(conv2d_size_out(conv2d_size_out(w))) * conv2d_size_out(conv2d_size_out(conv2d_size_out(h))) * 64\n",
    "        #print(linear_input_size)\n",
    "        self.dense1 = nn.Linear(linear_input_size, 512)\n",
    "        self.dense2 = nn.Linear(512, 256)\n",
    "        self.head = nn.Linear(256, outputs)\n",
    "        \n",
    "        ##### DELETE #####\n",
    "\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        #def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "        #    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        #convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        #convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        #linear_input_size = convw * convh * 32 ##### linear_input_size = 256\n",
    "        #self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            #--------------- USING BASIC ARCHITECTURE ---------------\n",
    "            #x = F.relu(self.bn1(self.conv1(x)))\n",
    "            #print('conv1.shape = ', x.shape)\n",
    "            #x = F.relu(self.bn2(self.conv2(x)))\n",
    "            #print('conv2.shape = ', x.shape)\n",
    "            #x = F.relu(self.bn3(self.conv3(x)))\n",
    "            #print('conv3.shape = ', x.shape)\n",
    "            \n",
    "            ##### DELETE #####\n",
    "            #x = x.view(x.size(0), -1) ##### Flattening ##### x = torch.flatten(x, 1)\n",
    "            #print('view = ', x.shape)\n",
    "            #x = F.relu(self.dense1(x))\n",
    "            #print('dense1.shape = ', x.shape)\n",
    "            #x = F.relu(self.dense2(x))\n",
    "            #print('dense2.shape = ', x.shape)\n",
    "            #output = self.head(x)\n",
    "            #print('output = ', output)\n",
    "            #return output\n",
    "            \n",
    "            \n",
    "            #--------------- USING PRE-TRAINED RESNET18 ---------------\n",
    "            #print('init.shape =',x.shape)\n",
    "            x = self.rn18_fe.forward(x) ##### output shape is torch.Size([1, 64, 15, 10])\n",
    "            #print('resnet18.shape =',x.shape)\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            #print('conv1.shape = ', x.shape)\n",
    "            x = F.relu(self.bn2(self.conv2(x)))\n",
    "            \n",
    "            #print('conv2.shape = ', x.shape)\n",
    "            #x = F.relu(self.bn3(self.conv3(x)))\n",
    "            #print('conv3.shape = ', x.shape)\n",
    "            \n",
    "            x = x.view(x.size(0), -1)\n",
    "            #print('view = ', x.shape)\n",
    "            x = F.relu(self.dense1(x))\n",
    "            #print('dense1.shape = ', x.shape)\n",
    "            x = F.relu(self.dense2(x))\n",
    "            #print('dense2.shape = ', x.shape)\n",
    "            output = self.head(x)\n",
    "            #print('output = ', output)\n",
    "            return output\n",
    "            \n",
    "            ##### DELETE #####\n",
    "            \n",
    "            #return self.head(x.view(x.size(0), -1)) ####### [batch_size, Ch*H*W] i.e. FLATENNING\n",
    "        except:\n",
    "            print(x)\n",
    "            print('--------------- EXCEPTION ---------------')\n",
    "            #return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        # if len(self.memory) > self.capacity:\n",
    "        #    self.clear()\n",
    "\n",
    "        # self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def clear(self):\n",
    "        # print(\"CLEARING MEMORY\")\n",
    "        self.position = 0\n",
    "        for m in self.memory:\n",
    "            del m\n",
    "        del self.memory\n",
    "        self.memory = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Model Class for Training and Testing\n",
    "\n",
    "\n",
    "class RL_Model():\n",
    "\n",
    "    # Creates a new RL Model, given a Gym Environment,\n",
    "    # NeuralNetwork Class and optional Action Space\n",
    "    def __init__(self, env, nn, action_space, env_string=None):\n",
    "        # set env\n",
    "        self.env = env\n",
    "        if env_string:\n",
    "            self.env_string = env_string\n",
    "            self.env = gym.make(env_string).unwrapped\n",
    "\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # initialize screen and nn\n",
    "        self.env.reset()\n",
    "        _, _, screen_height, screen_width = self.get_screen().shape\n",
    "\n",
    "        # set action space\n",
    "        if(action_space):\n",
    "            self.action_space = action_space\n",
    "        else:\n",
    "            self.action_space = env.action_space\n",
    "\n",
    "        # policy net\n",
    "        self.policy = nn(screen_height, screen_width,\n",
    "                         len(self.action_space)).to(self.device) ##### Height = 60, Width = 40\n",
    "        summary(self.policy, input_size=(3, screen_height, screen_width)) ###### DELETE ######\n",
    "\n",
    "        # target net\n",
    "        self.target = nn(screen_height, screen_width,\n",
    "                         len(self.action_space)).to(self.device)\n",
    "        summary(self.target, input_size=(3, screen_height, screen_width)) ###### DELETE ######\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = optim.RMSprop(self.policy.parameters())\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "        # variables for training\n",
    "        self.steps_taken = 0\n",
    "\n",
    "        self.episode_durations = []\n",
    "\n",
    "    # load policy-net weights\n",
    "    def load(self, path=\"rl_model_weights.pth\"):\n",
    "        checkpoint = torch.load(path)\n",
    "        # policy net\n",
    "        self.policy.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.policy.eval()\n",
    "\n",
    "        # target net\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # save policy-net weights\n",
    "    def save(self, path=\"rl_model_weights\"):\n",
    "        torch.save({\n",
    "            # 'epoch': epoch,\n",
    "            'model_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            # 'loss': loss,\n",
    "        },\n",
    "            path + \".pth\")\n",
    "\n",
    "    def get_screen(self):\n",
    "        screen = self.env.render(mode='rgb_array')\n",
    "        screen = screen[np.ix_([x for x in range(100, 400)], [\n",
    "                               x for x in range(200, 400)])]\n",
    "        screen = screen.transpose((2, 0, 1))\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 ######## screen.shape = (3x300x200)\n",
    "        screen = torch.from_numpy(screen) ######## torch.Size([3, 300, 200])\n",
    "        return resize(screen).unsqueeze(0).to(self.device) ####### torch.Size([1, 3, 60, 40])\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_taken / EPS_DECAY)\n",
    "        self.steps_taken += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(len(self.action_space))]], device=self.device, dtype=torch.long)\n",
    "\n",
    "    def select_deterministic_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            da = self.policy(state).max(1)[1].view(1, 1) ###### DELETE ######\n",
    "            #print('action =', da) ###### DELETE ######\n",
    "            return da\n",
    "            #return self.policy(state).max(1)[1].view(1, 1)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(\n",
    "            lambda s: s is not None, batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None])\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = self.policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (\n",
    "            next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # del non_final_mask\n",
    "        # del non_final_next_states\n",
    "        # del state_batch\n",
    "        # del action_batch\n",
    "        # del reward_batch\n",
    "        # del loss\n",
    "        # gc.collect()\n",
    "\n",
    "    def plot_durations(self):\n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Duration')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "    def train(self, num_episodes=NUM_TRAINING_EPISODES, epoch=1, render=False):\n",
    "        self.steps_taken = 0\n",
    "        acc_rewards = np.zeros(num_episodes)\n",
    "        plt_color = [(random.random(), random.random(), random.random())]\n",
    "        for i_ep in range(1, num_episodes+1):\n",
    "            #print(\"EPOCH: \" + str(epoch) + \" EPISODE: \" + str(i_ep))\n",
    "            #print(\"MEM ALLOCATED: \" + str(torch.cuda.memory_allocated()))\n",
    "            #print(\"MEM CACHE: \" + str(torch.cuda.memory_reserved()))\n",
    "            #print('Ram Used: %f' % memory_used())\n",
    "\n",
    "            # clear env\n",
    "            if i_ep % ENV_CLEAR == 0 and self.env_string:\n",
    "                self.env.close()\n",
    "                del self.env\n",
    "                self.env = gym.make(self.env_string).unwrapped\n",
    "\n",
    "            # reset env and state\n",
    "            self.env.reset()\n",
    "            last_screen = self.get_screen()\n",
    "            current_screen = self.get_screen()\n",
    "            state = current_screen - last_screen\n",
    "            \n",
    "            ### MEASUREMENTS ###\n",
    "            cntr = 0\n",
    "\n",
    "            for t in count():\n",
    "                if render:\n",
    "                    plt.imshow(self.get_screen().cpu().squeeze(\n",
    "                        0).permute(1, 2, 0).numpy(), interpolation='none')\n",
    "                    plt.draw()\n",
    "                    plt.pause(1e-3)\n",
    "\n",
    "                # select an action from the state\n",
    "                action = self.select_deterministic_action(state)\n",
    "                _, reward, done, _ = self.env.step(\n",
    "                    self.action_space[action.item()])\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "\n",
    "                # increase acc rewards\n",
    "                acc_rewards[i_ep-1] += reward\n",
    "\n",
    "                # observe new state\n",
    "                last_screen = current_screen\n",
    "                current_screen = self.get_screen()\n",
    "                # if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "                # else:\n",
    "                #    next_state = None\n",
    "\n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one optimization step on the target network\n",
    "                self.optimize_model()\n",
    "                \n",
    "                cntr += 1 #### \n",
    "\n",
    "                # Check if past step limit\n",
    "                if t > MAX_EPISODE_TIME:\n",
    "                    break\n",
    "\n",
    "            ### PRINTING PERFORMANCE ###\n",
    "            print('Episode:', i_ep, ' '*3,'Total_reward: %.2f' % acc_rewards[i_ep-1], ' '*3, \\\n",
    "                 'Average_RPE: %.2f' % (acc_rewards[i_ep-1] / cntr))\n",
    "            \n",
    "            \n",
    "            # Update target network, copy all weights and biases\n",
    "            if i_ep % TARGET_UPDATE == 0:\n",
    "                self.target.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "            # plot\n",
    "            #plt.title('Rewards Over Episode')\n",
    "            #plt.xlabel('Episode')\n",
    "            #plt.ylabel('Rewards')\n",
    "            #plt.xticks(range(1, epoch * (num_episodes+1)))\n",
    "            #plt.scatter(torch.tensor([i_ep + (epoch - 1) * num_episodes], dtype=torch.float), torch.tensor(\n",
    "            #    [max(acc_rewards[i_ep-1], -1000)], dtype=torch.float), c=plt_color, label=\"Epoch \" + str(epoch) if i_ep == 1 else '')\n",
    "            #plt.legend()\n",
    "            #plt.draw()\n",
    "            #plt.pause(1e-3)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        return (range(1, num_episodes+1), acc_rewards)\n",
    "\n",
    "    def generate_policy_video(self, filename=\"rl_model\", num_episodes=1, fps=30, max_episode_time=MAX_EPISODE_TIME):\n",
    "        filename = filename + \".mp4\"\n",
    "        with imageio.get_writer(filename, fps=fps) as video:\n",
    "            for episode in range(num_episodes):\n",
    "                time_step = self.env.reset()\n",
    "                done = False\n",
    "                video.append_data(self.env.render(mode=\"rgb_array\"))\n",
    "                last_screen = self.get_screen()\n",
    "                current_screen = self.get_screen()\n",
    "                state = current_screen - last_screen\n",
    "                \n",
    "\n",
    "                for i in range(max_episode_time):\n",
    "                    #action = self.select_deterministic_action(state)\n",
    "                    action = self.select_action(state)\n",
    "                    _, reward, done, _ = self.env.step(\n",
    "                        self.action_space[action.item()])\n",
    "                    video.append_data(self.env.render(mode=\"rgb_array\"))\n",
    "                    last_screen = current_screen\n",
    "                    current_screen = self.get_screen()\n",
    "                    state = current_screen-last_screen\n",
    "\n",
    "                    if(done):\n",
    "                        break\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashka\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "C:\\Users\\ashka\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1196..1499 -> 303-tiles track\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─rn18_feature_extractor: 1-1            --\n",
      "|    └─Sequential: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  9,408\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    |    └─MaxPool2d: 3-4               --\n",
      "├─Conv2d: 1-2                            204,928\n",
      "├─BatchNorm2d: 1-3                       256\n",
      "├─Conv2d: 1-4                            295,168\n",
      "├─BatchNorm2d: 1-5                       512\n",
      "├─Linear: 1-6                            262,656\n",
      "├─Linear: 1-7                            131,328\n",
      "├─Linear: 1-8                            2,056\n",
      "=================================================================\n",
      "Total params: 906,440\n",
      "Trainable params: 906,440\n",
      "Non-trainable params: 0\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─rn18_feature_extractor: 1-1            --\n",
      "|    └─Sequential: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  9,408\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    |    └─MaxPool2d: 3-4               --\n",
      "├─Conv2d: 1-2                            204,928\n",
      "├─BatchNorm2d: 1-3                       256\n",
      "├─Conv2d: 1-4                            295,168\n",
      "├─BatchNorm2d: 1-5                       512\n",
      "├─Linear: 1-6                            262,656\n",
      "├─Linear: 1-7                            131,328\n",
      "├─Linear: 1-8                            2,056\n",
      "=================================================================\n",
      "Total params: 906,440\n",
      "Trainable params: 906,440\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('CarRacing-v0').unwrapped\n",
    "\n",
    "discrete_action_space = {\"turn_left\": [-1, 0, 0], \"turn_right\": [1, 0, 0], \"go\": [0, 1, 0], \"go_left\": [-1,\n",
    "                                                                                                        1, 0], \"go_right\": [1, 1, 0], \"brake\": [0, 0, 1], \"brake_left\": [-1, 0, 1], \"brake_right\": [1, 0, 1]}\n",
    "d_actions = list(discrete_action_space.values())\n",
    "\n",
    "model = RL_Model(gym.make('CarRacing-v0').unwrapped,\n",
    "                 DQN, d_actions, 'CarRacing-v0')\n",
    "\n",
    "# model.generate_policy_video(\"rl_progress_ep_\" + str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 983..1232 -> 249-tiles track\n",
      "Episode: 1     Total_reward: -80.04     Average_RPE: -0.08\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Episode: 2     Total_reward: -69.48     Average_RPE: -0.07\n",
      "Track generation: 1115..1400 -> 285-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1346..1687 -> 341-tiles track\n",
      "Episode: 3     Total_reward: -73.73     Average_RPE: -0.07\n",
      "Track generation: 1111..1393 -> 282-tiles track\n",
      "Episode: 4     Total_reward: -71.73     Average_RPE: -0.07\n",
      "Track generation: 1156..1449 -> 293-tiles track\n",
      "Episode: 5     Total_reward: -38.56     Average_RPE: -0.04\n",
      "Track generation: 1169..1474 -> 305-tiles track\n",
      "Episode: 6     Total_reward: -73.88     Average_RPE: -0.07\n",
      "Track generation: 981..1237 -> 256-tiles track\n",
      "Episode: 7     Total_reward: -60.98     Average_RPE: -0.06\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Episode: 8     Total_reward: -52.42     Average_RPE: -0.05\n",
      "Track generation: 1199..1503 -> 304-tiles track\n",
      "Episode: 9     Total_reward: -37.49     Average_RPE: -0.04\n",
      "Track generation: 1117..1400 -> 283-tiles track\n",
      "Episode: 10     Total_reward: -68.28     Average_RPE: -0.07\n",
      "Track generation: 1201..1506 -> 305-tiles track\n",
      "Episode: 11     Total_reward: -77.17     Average_RPE: -0.08\n",
      "Track generation: 1102..1382 -> 280-tiles track\n",
      "Episode: 12     Total_reward: -67.94     Average_RPE: -0.07\n",
      "Track generation: 1084..1364 -> 280-tiles track\n",
      "Episode: 13     Total_reward: -46.44     Average_RPE: -0.05\n",
      "Track generation: 1301..1630 -> 329-tiles track\n",
      "Episode: 14     Total_reward: -63.61     Average_RPE: -0.06\n",
      "Track generation: 1145..1435 -> 290-tiles track\n",
      "Episode: 15     Total_reward: -41.38     Average_RPE: -0.04\n",
      "Track generation: 1176..1474 -> 298-tiles track\n",
      "Episode: 16     Total_reward: -63.16     Average_RPE: -0.06\n",
      "Track generation: 1059..1327 -> 268-tiles track\n",
      "Episode: 17     Total_reward: -62.75     Average_RPE: -0.06\n",
      "Track generation: 1177..1486 -> 309-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1091..1368 -> 277-tiles track\n",
      "Episode: 18     Total_reward: -71.21     Average_RPE: -0.07\n",
      "Track generation: 1223..1533 -> 310-tiles track\n",
      "Episode: 19     Total_reward: -71.07     Average_RPE: -0.07\n",
      "Track generation: 1158..1452 -> 294-tiles track\n",
      "Episode: 20     Total_reward: -72.90     Average_RPE: -0.07\n",
      "Track generation: 1025..1285 -> 260-tiles track\n",
      "Episode: 21     Total_reward: -69.31     Average_RPE: -0.07\n",
      "Track generation: 1155..1448 -> 293-tiles track\n",
      "Episode: 22     Total_reward: -65.95     Average_RPE: -0.07\n",
      "Track generation: 1178..1485 -> 307-tiles track\n",
      "Episode: 23     Total_reward: -70.79     Average_RPE: -0.07\n",
      "Track generation: 1125..1411 -> 286-tiles track\n",
      "Episode: 24     Total_reward: -33.53     Average_RPE: -0.03\n",
      "Track generation: 1115..1398 -> 283-tiles track\n",
      "Episode: 25     Total_reward: -71.83     Average_RPE: -0.07\n",
      "Track generation: 1212..1519 -> 307-tiles track\n",
      "Episode: 26     Total_reward: -34.84     Average_RPE: -0.03\n",
      "Track generation: 990..1248 -> 258-tiles track\n",
      "Episode: 27     Total_reward: -72.96     Average_RPE: -0.07\n",
      "Track generation: 1189..1490 -> 301-tiles track\n",
      "Episode: 28     Total_reward: -73.53     Average_RPE: -0.07\n",
      "Track generation: 935..1180 -> 245-tiles track\n",
      "Episode: 29     Total_reward: -71.51     Average_RPE: -0.07\n",
      "Track generation: 912..1144 -> 232-tiles track\n",
      "Episode: 30     Total_reward: -61.24     Average_RPE: -0.06\n",
      "Track generation: 1260..1579 -> 319-tiles track\n",
      "Episode: 31     Total_reward: -65.61     Average_RPE: -0.07\n",
      "Track generation: 1128..1414 -> 286-tiles track\n",
      "Episode: 32     Total_reward: -61.60     Average_RPE: -0.06\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "Episode: 33     Total_reward: -79.58     Average_RPE: -0.08\n",
      "Track generation: 1132..1427 -> 295-tiles track\n",
      "Episode: 34     Total_reward: -59.38     Average_RPE: -0.06\n",
      "Track generation: 1132..1419 -> 287-tiles track\n",
      "Episode: 35     Total_reward: -51.25     Average_RPE: -0.05\n",
      "Track generation: 1088..1364 -> 276-tiles track\n",
      "Episode: 36     Total_reward: -78.38     Average_RPE: -0.08\n",
      "Track generation: 1048..1319 -> 271-tiles track\n",
      "Episode: 37     Total_reward: -40.94     Average_RPE: -0.04\n",
      "Track generation: 1300..1629 -> 329-tiles track\n",
      "Episode: 38     Total_reward: -69.71     Average_RPE: -0.07\n",
      "Track generation: 933..1175 -> 242-tiles track\n",
      "Episode: 39     Total_reward: -58.71     Average_RPE: -0.06\n",
      "Track generation: 1055..1328 -> 273-tiles track\n",
      "Episode: 40     Total_reward: -70.79     Average_RPE: -0.07\n",
      "Track generation: 1080..1354 -> 274-tiles track\n",
      "Episode: 41     Total_reward: -23.28     Average_RPE: -0.02\n",
      "Track generation: 1096..1380 -> 284-tiles track\n",
      "Episode: 42     Total_reward: -43.66     Average_RPE: -0.04\n",
      "Track generation: 1234..1546 -> 312-tiles track\n",
      "Episode: 43     Total_reward: -77.69     Average_RPE: -0.08\n",
      "Track generation: 1204..1514 -> 310-tiles track\n",
      "Episode: 44     Total_reward: -71.07     Average_RPE: -0.07\n",
      "Track generation: 979..1228 -> 249-tiles track\n",
      "Episode: 45     Total_reward: 8.67     Average_RPE: 0.01\n",
      "Track generation: 1127..1422 -> 295-tiles track\n",
      "Episode: 46     Total_reward: -79.79     Average_RPE: -0.08\n",
      "Track generation: 1119..1403 -> 284-tiles track\n",
      "Episode: 47     Total_reward: -71.93     Average_RPE: -0.07\n",
      "Track generation: 1047..1323 -> 276-tiles track\n",
      "Episode: 48     Total_reward: -67.47     Average_RPE: -0.07\n",
      "Track generation: 1278..1602 -> 324-tiles track\n",
      "Episode: 49     Total_reward: -78.53     Average_RPE: -0.08\n",
      "Track generation: 1138..1427 -> 289-tiles track\n",
      "Episode: 50     Total_reward: -68.95     Average_RPE: -0.07\n",
      "Track generation: 1067..1338 -> 271-tiles track\n",
      "Episode: 51     Total_reward: -70.57     Average_RPE: -0.07\n",
      "Track generation: 969..1218 -> 249-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1245..1560 -> 315-tiles track\n",
      "Episode: 52     Total_reward: -81.09     Average_RPE: -0.08\n",
      "Track generation: 1043..1308 -> 265-tiles track\n",
      "Episode: 53     Total_reward: -73.68     Average_RPE: -0.07\n",
      "Track generation: 1273..1600 -> 327-tiles track\n",
      "Episode: 54     Total_reward: -81.79     Average_RPE: -0.08\n",
      "Track generation: 1114..1402 -> 288-tiles track\n",
      "Episode: 55     Total_reward: -75.81     Average_RPE: -0.08\n",
      "Track generation: 1055..1330 -> 275-tiles track\n",
      "Episode: 56     Total_reward: -74.65     Average_RPE: -0.07\n",
      "Track generation: 1296..1624 -> 328-tiles track\n",
      "Episode: 57     Total_reward: -75.73     Average_RPE: -0.08\n",
      "Track generation: 1024..1284 -> 260-tiles track\n",
      "Episode: 58     Total_reward: -73.17     Average_RPE: -0.07\n",
      "Track generation: 1336..1674 -> 338-tiles track\n",
      "Episode: 59     Total_reward: -76.46     Average_RPE: -0.08\n",
      "Track generation: 1228..1539 -> 311-tiles track\n",
      "Episode: 60     Total_reward: -67.94     Average_RPE: -0.07\n",
      "Track generation: 1081..1355 -> 274-tiles track\n",
      "Episode: 61     Total_reward: -67.23     Average_RPE: -0.07\n",
      "Track generation: 1376..1725 -> 349-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1162..1455 -> 293-tiles track\n",
      "Episode: 62     Total_reward: -52.25     Average_RPE: -0.05\n",
      "Track generation: 1048..1319 -> 271-tiles track\n",
      "Episode: 63     Total_reward: -77.98     Average_RPE: -0.08\n",
      "Track generation: 1139..1428 -> 289-tiles track\n",
      "Episode: 64     Total_reward: -75.89     Average_RPE: -0.08\n",
      "Track generation: 1195..1508 -> 313-tiles track\n",
      "Episode: 65     Total_reward: -77.76     Average_RPE: -0.08\n",
      "Track generation: 1019..1284 -> 265-tiles track\n",
      "Episode: 66     Total_reward: -69.90     Average_RPE: -0.07\n",
      "Track generation: 1244..1559 -> 315-tiles track\n",
      "Episode: 67     Total_reward: -71.54     Average_RPE: -0.07\n",
      "Track generation: 1135..1423 -> 288-tiles track\n",
      "Episode: 68     Total_reward: -72.32     Average_RPE: -0.07\n",
      "Track generation: 1259..1578 -> 319-tiles track\n",
      "Episode: 69     Total_reward: -71.90     Average_RPE: -0.07\n",
      "Track generation: 1258..1577 -> 319-tiles track\n",
      "Episode: 70     Total_reward: -71.90     Average_RPE: -0.07\n",
      "Track generation: 1038..1308 -> 270-tiles track\n",
      "Episode: 71     Total_reward: -70.46     Average_RPE: -0.07\n",
      "Track generation: 986..1239 -> 253-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1175..1473 -> 298-tiles track\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 72     Total_reward: -76.63     Average_RPE: -0.08\n",
      "Track generation: 1291..1616 -> 325-tiles track\n",
      "Episode: 73     Total_reward: -75.51     Average_RPE: -0.08\n",
      "Track generation: 1204..1509 -> 305-tiles track\n",
      "Episode: 74     Total_reward: -80.46     Average_RPE: -0.08\n",
      "Track generation: 1215..1523 -> 308-tiles track\n",
      "Episode: 75     Total_reward: -67.63     Average_RPE: -0.07\n",
      "Track generation: 1170..1472 -> 302-tiles track\n",
      "Episode: 76     Total_reward: -80.27     Average_RPE: -0.08\n",
      "Track generation: 991..1243 -> 252-tiles track\n",
      "Episode: 77     Total_reward: -72.31     Average_RPE: -0.07\n",
      "Track generation: 982..1231 -> 249-tiles track\n",
      "Episode: 78     Total_reward: -76.01     Average_RPE: -0.08\n",
      "Track generation: 1128..1414 -> 286-tiles track\n",
      "Episode: 79     Total_reward: -68.62     Average_RPE: -0.07\n",
      "Track generation: 1192..1494 -> 302-tiles track\n",
      "Episode: 80     Total_reward: -76.94     Average_RPE: -0.08\n",
      "Track generation: 1219..1528 -> 309-tiles track\n",
      "Episode: 81     Total_reward: -48.25     Average_RPE: -0.05\n",
      "Track generation: 1043..1308 -> 265-tiles track\n",
      "Episode: 82     Total_reward: -16.87     Average_RPE: -0.02\n",
      "Track generation: 1165..1467 -> 302-tiles track\n",
      "Episode: 83     Total_reward: -83.59     Average_RPE: -0.08\n",
      "Track generation: 1124..1409 -> 285-tiles track\n",
      "Episode: 84     Total_reward: -5.13     Average_RPE: -0.01\n",
      "Track generation: 1075..1348 -> 273-tiles track\n",
      "Episode: 85     Total_reward: -52.41     Average_RPE: -0.05\n",
      "Track generation: 1160..1454 -> 294-tiles track\n",
      "Episode: 86     Total_reward: -25.11     Average_RPE: -0.03\n",
      "Track generation: 1068..1340 -> 272-tiles track\n",
      "Episode: 87     Total_reward: -63.30     Average_RPE: -0.06\n",
      "Track generation: 1092..1369 -> 277-tiles track\n",
      "Episode: 88     Total_reward: -3364.29     Average_RPE: -3.36\n",
      "Track generation: 1203..1508 -> 305-tiles track\n",
      "Episode: 89     Total_reward: -37.70     Average_RPE: -0.04\n",
      "Track generation: 1056..1326 -> 270-tiles track\n",
      "Episode: 90     Total_reward: -59.31     Average_RPE: -0.06\n",
      "Track generation: 940..1179 -> 239-tiles track\n",
      "Episode: 91     Total_reward: -32.97     Average_RPE: -0.03\n",
      "Track generation: 1124..1409 -> 285-tiles track\n",
      "Episode: 92     Total_reward: -61.47     Average_RPE: -0.06\n",
      "Track generation: 1007..1263 -> 256-tiles track\n",
      "Episode: 93     Total_reward: -6.08     Average_RPE: -0.01\n",
      "Track generation: 1200..1504 -> 304-tiles track\n",
      "Episode: 94     Total_reward: -34.19     Average_RPE: -0.03\n",
      "Track generation: 1033..1298 -> 265-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1138..1431 -> 293-tiles track\n",
      "Episode: 95     Total_reward: -48.83     Average_RPE: -0.05\n",
      "Track generation: 1076..1353 -> 277-tiles track\n",
      "Episode: 96     Total_reward: -49.48     Average_RPE: -0.05\n",
      "Track generation: 1117..1401 -> 284-tiles track\n",
      "Episode: 97     Total_reward: -57.80     Average_RPE: -0.06\n",
      "Track generation: 1099..1378 -> 279-tiles track\n",
      "Episode: 98     Total_reward: -57.03     Average_RPE: -0.06\n",
      "Track generation: 1014..1277 -> 263-tiles track\n",
      "Episode: 99     Total_reward: -77.30     Average_RPE: -0.08\n",
      "Track generation: 1120..1404 -> 284-tiles track\n",
      "Episode: 100     Total_reward: -64.86     Average_RPE: -0.06\n",
      "Track generation: 1276..1607 -> 331-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    ep, rewards = model.train(\n",
    "        100, render=False, epoch=i)\n",
    "    model.save(\"rl_progress_ep_\" + str(i * 50))\n",
    "    model.generate_policy_video(\"rl_progress_ep_\" + str(i*50))\n",
    "plt.savefig(\"rl_progress_plt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
